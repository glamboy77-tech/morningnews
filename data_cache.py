import json
import os
from datetime import datetime
from config import config

class DataCache:
    def __init__(self):
        self.cache_dir = "data_cache"
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def get_cache_filename(self, cache_type, date_str=None):
        """
        ìºì‹œ íŒŒì¼ëª… ìƒì„±
        cache_type: 'rss', 'ai_analysis', 'key_persons'
        """
        if date_str is None:
            date_str = datetime.now().strftime("%Y%m%d")
        return os.path.join(self.cache_dir, f"{cache_type}_{date_str}.json")
    
    def save_rss_data(self, rss_data, date_str=None):
        """RSS ìˆ˜ì§‘ ë°ì´í„° ìºì‹œ ì €ì¥"""
        cache_file = self.get_cache_filename('rss', date_str)
        tmp_file = f"{cache_file}.tmp"
        try:
            cache_data = {
                "timestamp": datetime.now().isoformat(),
                "date": date_str or datetime.now().strftime("%Y%m%d"),
                "data": rss_data
            }
            with open(tmp_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2, default=str)
            os.replace(tmp_file, cache_file)
            print(f"âœ… RSS ë°ì´í„° ìºì‹œ ì €ì¥: {cache_file}")
            return True
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"âŒ RSS ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_rss_data(self, date_str=None):
        """RSS ìˆ˜ì§‘ ë°ì´í„° ìºì‹œ ë¡œë“œ"""
        cache_file = self.get_cache_filename('rss', date_str)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                print(f"âœ… RSS ë°ì´í„° ìºì‹œ ë¡œë“œ: {cache_file}")
                loaded_data = cache_data.get("data", [])
                # datetime ê°ì²´ ë³µì›
                for item in loaded_data:
                    if 'published_dt' in item and isinstance(item['published_dt'], str):
                        item['published_dt'] = datetime.fromisoformat(item['published_dt'])
                return loaded_data
            except Exception as e:
                print(f"âŒ RSS ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def save_ai_analysis(self, ai_data, date_str=None):
        """AI ë¶„ì„ ë°ì´í„° ìºì‹œ ì €ì¥"""
        cache_file = self.get_cache_filename('ai_analysis', date_str)
        tmp_file = f"{cache_file}.tmp"
        try:
            cache_data = {
                "timestamp": datetime.now().isoformat(),
                "date": date_str or datetime.now().strftime("%Y%m%d"),
                "data": ai_data
            }
            with open(tmp_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2, default=str)
            os.replace(tmp_file, cache_file)
            print(f"âœ… AI ë¶„ì„ ë°ì´í„° ìºì‹œ ì €ì¥: {cache_file}")
            return True
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"âŒ AI ë¶„ì„ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_ai_analysis(self, date_str=None):
        """AI ë¶„ì„ ë°ì´í„° ìºì‹œ ë¡œë“œ"""
        cache_file = self.get_cache_filename('ai_analysis', date_str)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                print(f"âœ… AI ë¶„ì„ ë°ì´í„° ìºì‹œ ë¡œë“œ: {cache_file}")
                loaded_data = cache_data.get("data", {})
                # datetime ê°ì²´ ë³µì›
                for category, items in loaded_data.items():
                    for item in items:
                        if 'published_dt' in item and isinstance(item['published_dt'], str):
                            item['published_dt'] = datetime.fromisoformat(item['published_dt'])
                return loaded_data
            except Exception as e:
                print(f"âŒ AI ë¶„ì„ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def save_key_persons(self, key_persons, date_str=None):
        """ì£¼ìš” ì¸ë¬¼ ì¶”ì¶œ ë°ì´í„° ìºì‹œ ì €ì¥"""
        cache_file = self.get_cache_filename('key_persons', date_str)
        tmp_file = f"{cache_file}.tmp"
        try:
            cache_data = {
                "timestamp": datetime.now().isoformat(),
                "date": date_str or datetime.now().strftime("%Y%m%d"),
                "data": key_persons
            }
            with open(tmp_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2, default=str)
            os.replace(tmp_file, cache_file)
            print(f"âœ… ì£¼ìš” ì¸ë¬¼ ë°ì´í„° ìºì‹œ ì €ì¥: {cache_file}")
            return True
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"âŒ ì£¼ìš” ì¸ë¬¼ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_key_persons(self, date_str=None):
        """ì£¼ìš” ì¸ë¬¼ ì¶”ì¶œ ë°ì´í„° ìºì‹œ ë¡œë“œ"""
        cache_file = self.get_cache_filename('key_persons', date_str)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                print(f"âœ… ì£¼ìš” ì¸ë¬¼ ë°ì´í„° ìºì‹œ ë¡œë“œ: {cache_file}")
                return cache_data.get("data", {})
            except Exception as e:
                print(f"âŒ ì£¼ìš” ì¸ë¬¼ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def has_cache(self, cache_type, date_str=None):
        """íŠ¹ì • íƒ€ì…ì˜ ìºì‹œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        cache_file = self.get_cache_filename(cache_type, date_str)
        return os.path.exists(cache_file)
    
    def get_cache_status(self, date_str=None):
        """ì˜¤ëŠ˜ì˜ ìºì‹œ ìƒíƒœ í™•ì¸"""
        if date_str is None:
            date_str = datetime.now().strftime("%Y%m%d")
        
        status = {
            "date": date_str,
            "rss": self.has_cache('rss', date_str),
            "ai_analysis": self.has_cache('ai_analysis', date_str),
            "key_persons": self.has_cache('key_persons', date_str),
            "all_complete": False
        }
        
        status["all_complete"] = all([
            status["rss"],
            status["ai_analysis"], 
            status["key_persons"]
        ])
        
        return status
    
    def clear_cache(self, cache_type=None, date_str=None):
        """ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        if cache_type:
            # íŠ¹ì • íƒ€ì…ë§Œ ì •ë¦¬
            cache_file = self.get_cache_filename(cache_type, date_str)
            if os.path.exists(cache_file):
                try:
                    os.remove(cache_file)
                    print(f"ğŸ—‘ï¸ ìºì‹œ íŒŒì¼ ì •ë¦¬: {cache_file}")
                    return True
                except Exception as e:
                    print(f"âŒ ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        else:
            # ëª¨ë“  ìºì‹œ ì •ë¦¬
            if date_str is None:
                date_str = datetime.now().strftime("%Y%m%d")
            
            for cache_type in ['rss', 'ai_analysis', 'key_persons']:
                self.clear_cache(cache_type, date_str)
        
        return False
    
    def list_cache_files(self):
        """ìºì‹œ íŒŒì¼ ëª©ë¡ í™•ì¸"""
        cache_files = []
        if os.path.exists(self.cache_dir):
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json'):
                    filepath = os.path.join(self.cache_dir, filename)
                    file_info = {
                        "filename": filename,
                        "filepath": filepath,
                        "size": os.path.getsize(filepath),
                        "modified": datetime.fromtimestamp(os.path.getmtime(filepath)).isoformat()
                    }
                    cache_files.append(file_info)
        
        return sorted(cache_files, key=lambda x: x["modified"], reverse=True)

if __name__ == "__main__":
    # DataCache í…ŒìŠ¤íŠ¸
    cache = DataCache()
    
    print("ğŸ§ª DataCache í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_rss = [
        {"title": "í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤ 1", "source": "í…ŒìŠ¤íŠ¸", "published_dt": "2026-01-17"},
        {"title": "í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤ 2", "source": "í…ŒìŠ¤íŠ¸", "published_dt": "2026-01-17"}
    ]
    
    test_ai = {
        "ì •ì¹˜": [{"title": "í…ŒìŠ¤íŠ¸ ì •ì¹˜", "source": "í…ŒìŠ¤íŠ¸"}],
        "ê²½ì œ/ê±°ì‹œ": [{"title": "í…ŒìŠ¤íŠ¸ ê²½ì œ", "source": "í…ŒìŠ¤íŠ¸"}]
    }
    
    test_persons = {
        "í…ŒìŠ¤íŠ¸ ì¸ë¬¼": {"articles": [], "count": 5, "role": "í…ŒìŠ¤íŠ¸ ì—­í• "}
    }
    
    # ì €ì¥ í…ŒìŠ¤íŠ¸
    cache.save_rss_data(test_rss)
    cache.save_ai_analysis(test_ai)
    cache.save_key_persons(test_persons)
    
    # ë¡œë“œ í…ŒìŠ¤íŠ¸
    loaded_rss = cache.load_rss_data()
    loaded_ai = cache.load_ai_analysis()
    loaded_persons = cache.load_key_persons()
    
    print(f"âœ… RSS ë¡œë“œ: {'ì„±ê³µ' if loaded_rss else 'ì‹¤íŒ¨'}")
    print(f"âœ… AI ë¶„ì„ ë¡œë“œ: {'ì„±ê³µ' if loaded_ai else 'ì‹¤íŒ¨'}")
    print(f"âœ… ì£¼ìš” ì¸ë¬¼ ë¡œë“œ: {'ì„±ê³µ' if loaded_persons else 'ì‹¤íŒ¨'}")
    
    # ìƒíƒœ í™•ì¸
    status = cache.get_cache_status()
    print(f"ğŸ“Š ìºì‹œ ìƒíƒœ: {status}")
    
    # íŒŒì¼ ëª©ë¡
    files = cache.list_cache_files()
    print(f"ğŸ“ ìºì‹œ íŒŒì¼: {len(files)}ê°œ")
    
    print("ğŸ‰ DataCache í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
