from google import genai
import json
import os
import time
import glob
from datetime import datetime, timedelta
from config import config

class SentimentAnalyzer:
    def __init__(self):
        self.client = genai.Client(api_key=config.gemini_api_key)
        self.cache_dir = "sentiment_cache"
        os.makedirs(self.cache_dir, exist_ok=True)

    # -----------------------------
    # Cache helpers
    # -----------------------------
        
    def is_trading_day(self):
        """
        í˜„ì¬ê°€ í‰ì¼ ì¥ ì¤‘ì¸ì§€, ì•„ë‹ˆë©´ ì£¼ë§/ê³µíœ´ì¼ì¸ì§€ íŒë‹¨
        Returns: 'trading' (í‰ì¼ ì¥ ì¤‘) or 'accumulation' (ì£¼ë§/ê³µíœ´ì¼)
        """
        now = datetime.now()
        
        # ì£¼ë§ ì²´í¬
        if now.weekday() >= 5:  # í† ìš”ì¼(5), ì¼ìš”ì¼(6)
            return 'accumulation'
        
        # ê³µíœ´ì¼ ì²´í¬ (ê°„ë‹¨í•œ í•œêµ­ ê³µíœ´ì¼ ëª©ë¡)
        month = now.month
        day = now.day
        
        korean_holidays = {
            (1, 1): "ì‹ ì •",
            (3, 1): "ì‚¼ì¼ì ˆ",
            (5, 5): "ì–´ë¦°ì´ë‚ ",
            (6, 6): "í˜„ì¶©ì¼",
            (8, 15): "ê´‘ë³µì ˆ",
            (10, 3): "ê°œì²œì ˆ",
            (10, 9): "í•œê¸€ë‚ ",
            (12, 25): "ì„±íƒ„ì ˆ"
        }
        
        if (month, day) in korean_holidays:
            return 'accumulation'
        
        # ì¥ ì‹œê°„ ì²´í¬ (í‰ì¼ 9:00-15:30)
        if 9 <= now.hour < 16:
            return 'trading'
        else:
            return 'accumulation'
    
    def is_first_trading_day_after_holiday(self):
        """
        ì›”ìš”ì¼ ë˜ëŠ” ê³µíœ´ì¼ ë‹¤ìŒ ë‚ ì¸ì§€ íŒë‹¨
        Returns: True if today is the first trading day after a holiday/weekend
        """
        now = datetime.now()
        
        # ì›”ìš”ì¼ ì²´í¬
        if now.weekday() == 0:  # 0ì€ ì›”ìš”ì¼
            return True
        
        # ì–´ì œê°€ ê³µíœ´ì¼ì´ì—ˆëŠ”ì§€ ì²´í¬
        yesterday = now - timedelta(days=1)
        yesterday_month = yesterday.month
        yesterday_day = yesterday.day
        
        korean_holidays = {
            (1, 1): "ì‹ ì •",
            (3, 1): "ì‚¼ì¼ì ˆ",
            (5, 5): "ì–´ë¦°ì´ë‚ ",
            (6, 6): "í˜„ì¶©ì¼",
            (8, 15): "ê´‘ë³µì ˆ",
            (10, 3): "ê°œì²œì ˆ",
            (10, 9): "í•œê¸€ë‚ ",
            (12, 25): "ì„±íƒ„ì ˆ"
        }
        
        # ì–´ì œê°€ ì£¼ë§ì´ê±°ë‚˜ ê³µíœ´ì¼ì´ì—ˆìœ¼ë©´ True
        if yesterday.weekday() >= 5 or (yesterday_month, yesterday_day) in korean_holidays:
            return True
        
        return False
    
    def get_time_weight(self, news_datetime):
        """
        ë‰´ìŠ¤ ì‹œê°„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê³„ì‚°
        - ì–´ì œ 15:30 ~ ì˜¤ëŠ˜ 08:30: ê°€ì¤‘ì¹˜ 2.0 (ì¥ì™¸ ë‰´ìŠ¤)
        - ì–´ì œ 09:00 ~ 15:30: ê°€ì¤‘ì¹˜ 1.0 (ì¥ì¤‘ ë‰´ìŠ¤)
        - ê·¸ ì™¸: ê°€ì¤‘ì¹˜ 0.5 (ê¸°íƒ€)
        """
        now = datetime.now()
        yesterday = now - timedelta(days=1)
        
        # ì¥ì™¸ ì‹œê°„ëŒ€: ì–´ì œ 15:30 ~ ì˜¤ëŠ˜ 08:30
        market_close_yesterday = yesterday.replace(hour=15, minute=30, second=0, microsecond=0)
        market_open_today = now.replace(hour=8, minute=30, second=0, microsecond=0)
        
        # ì¥ì¤‘ ì‹œê°„ëŒ€: ì–´ì œ 09:00 ~ 15:30
        market_open_yesterday = yesterday.replace(hour=9, minute=0, second=0, microsecond=0)
        
        if market_close_yesterday <= news_datetime <= market_open_today:
            return 2.0  # ì¥ì™¸ ë‰´ìŠ¤ (ê°€ì¥ ì¤‘ìš”)
        elif market_open_yesterday <= news_datetime < market_close_yesterday:
            return 1.0  # ì¥ì¤‘ ë‰´ìŠ¤ (ì´ë¯¸ ë°˜ì˜ë¨)
        else:
            return 0.5  # ê¸°íƒ€ ë‰´ìŠ¤
    
    def filter_trading_signals(self, categorized_news):
        """
        ë§¤ë§¤ë´‡ìš© í˜¸ì¬/ì•…ì¬ í•„í„°ë§
        - ì¥ì™¸ ë‰´ìŠ¤(ì–´ì œ 15:30~ì˜¤ëŠ˜ 08:30)ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        """
        filtered_news = {}
        
        for category, items in categorized_news.items():
            filtered_items = []
            
            for item in items:
                news_time = item.get('published_dt')
                if news_time:
                    weight = self.get_time_weight(news_time)
                    # ê°€ì¤‘ì¹˜ê°€ 1.0 ì´ìƒì¸ ë‰´ìŠ¤ë§Œ í¬í•¨ (ì¥ì™¸ + ì¥ì¤‘)
                    if weight >= 1.0:
                        item['time_weight'] = weight
                        filtered_items.append(item)
            
            if filtered_items:
                filtered_news[category] = filtered_items
        
        return filtered_news
    
    def get_cache_filename(self, date_str=None):
        """
        ìºì‹œ íŒŒì¼ëª… ìƒì„±
        """
        if date_str is None:
            date_str = datetime.now().strftime("%Y%m%d")
        return os.path.join(self.cache_dir, f"sentiment_{date_str}.json")

    def _find_latest_cache_file(self, exclude_date_str=None, max_age_days: int | None = 2):
        """Find newest sentiment_YYYYMMDD.json.

        Args:
            exclude_date_str: íŠ¹ì • ë‚ ì§œ(YYYYMMDD)ëŠ” ì œì™¸
            max_age_days: ì˜¤ëŠ˜ ê¸°ì¤€ ìµœëŒ€ ëª‡ ì¼ ì „ê¹Œì§€ í—ˆìš©í• ì§€. Noneì´ë©´ ì œí•œ ì—†ìŒ.

        Returns:
            (date_str, path) or (None, None)
        """
        candidates = sorted(glob.glob(os.path.join(self.cache_dir, "sentiment_*.json")))
        # Sort by filename date then mtime as tie-breaker
        def _key(p):
            base = os.path.basename(p)
            m = base.replace("sentiment_", "").replace(".json", "")
            try:
                dt = datetime.strptime(m, "%Y%m%d")
            except Exception:
                dt = datetime.fromtimestamp(os.path.getmtime(p))
            return (dt, os.path.getmtime(p))
        candidates.sort(key=_key, reverse=True)

        now = datetime.now()
        for path in candidates:
            base = os.path.basename(path)
            date_str = base.replace("sentiment_", "").replace(".json", "")
            if exclude_date_str and date_str == exclude_date_str:
                continue

            if max_age_days is not None:
                try:
                    dt = datetime.strptime(date_str, "%Y%m%d")
                    if (now - dt).days > max_age_days:
                        continue
                except Exception:
                    # If we can't parse date, skip it when age limiting is enabled
                    continue

            return date_str, path
        return None, None
    
    def load_cached_data(self, date_str=None):
        """
        ìºì‹œëœ ë°ì´í„° ë¡œë“œ
        """
        cache_file = self.get_cache_filename(date_str)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def save_cached_data(self, data, date_str=None):
        """
        ë°ì´í„° ìºì‹œ ì €ì¥
        """
        cache_file = self.get_cache_filename(date_str)
        tmp_file = f"{cache_file}.tmp"
        try:
            with open(tmp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            os.replace(tmp_file, cache_file)
            print(f"âœ… ê°ì„± ë°ì´í„° ìºì‹œ ì €ì¥: {cache_file}")
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # -----------------------------
    # Retry helpers
    # -----------------------------
    @staticmethod
    def _is_retryable_error(err: Exception) -> bool:
        msg = str(err)
        # Gemini/Google GenAI commonly returns "503 UNAVAILABLE" when overloaded.
        return (
            "503" in msg
            or "UNAVAILABLE" in msg
            or "overloaded" in msg.lower()
            or "rate" in msg.lower() and "limit" in msg.lower()
        )

    def _generate_json_with_retry(self, prompt: str, *, model: str, max_retries: int = 3, base_sleep_sec: float = 2.0):
        last_err = None
        for attempt in range(1, max_retries + 1):
            try:
                resp = self.client.models.generate_content(
                    model=model,
                    contents=prompt,
                    config={'response_mime_type': 'application/json'}
                )
                return json.loads(resp.text)
            except Exception as e:
                last_err = e
                if attempt < max_retries and self._is_retryable_error(e):
                    sleep_sec = base_sleep_sec * (2 ** (attempt - 1))
                    print(f"âš ï¸ Gemini í˜¸ì¶œ ì‹¤íŒ¨(ì¬ì‹œë„ {attempt}/{max_retries}): {e}")
                    print(f"   -> {sleep_sec:.1f}s í›„ ì¬ì‹œë„")
                    time.sleep(sleep_sec)
                    continue
                raise

        # Should never reach here
        raise last_err
    
    def merge_sentiment_data(self, current_data, cached_data):
        """
        í˜„ì¬ ë°ì´í„°ì™€ ìºì‹œëœ ë°ì´í„°ë¥¼ ë³‘í•©
        - ë™ì¼ ì¢…ëª©ì€ ì ìˆ˜ í•©ì‚°
        - ìƒˆë¡œìš´ ì¢…ëª©ì€ ì¶”ê°€
        """
        if not cached_data:
            return current_data
        
        merged = {
            "section_summaries": cached_data.get("section_summaries", {}),
            "hojae": [],
            "akjae": [],
            "merged_dates": cached_data.get("merged_dates", [])
        }
        
        # í˜„ì¬ ë‚ ì§œ ì¶”ê°€
        today = datetime.now().strftime("%Y-%m-%d")
        if today not in merged["merged_dates"]:
            merged["merged_dates"].append(today)
        
        # í˜¸ì¬ ë°ì´í„° ë³‘í•©
        hojae_dict = {}
        
        # ìºì‹œëœ í˜¸ì¬ ë°ì´í„° ì²˜ë¦¬
        for item in cached_data.get("hojae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                hojae_dict[company.strip()] = {
                    "reason": reason.strip(),
                    "count": 1,
                    "score": 1
                }
        
        # í˜„ì¬ í˜¸ì¬ ë°ì´í„° ì²˜ë¦¬ ë° ë³‘í•©
        for item in current_data.get("hojae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                company = company.strip()
                if company in hojae_dict:
                    hojae_dict[company]["count"] += 1
                    hojae_dict[company]["score"] += 1
                    # ìµœì‹  ì´ìœ ë¡œ ì—…ë°ì´íŠ¸
                    hojae_dict[company]["reason"] = reason.strip()
                else:
                    hojae_dict[company] = {
                        "reason": reason.strip(),
                        "count": 1,
                        "score": 1
                    }
        
        # ë³‘í•©ëœ í˜¸ì¬ ë°ì´í„° ìƒì„±
        for company, data in hojae_dict.items():
            merged["hojae"].append(f"{company}: {data['reason']} ({data['count']}íšŒ)")
        
        # ì•…ì¬ ë°ì´í„° ë³‘í•© (ë™ì¼ ë¡œì§)
        akjae_dict = {}
        
        for item in cached_data.get("akjae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                akjae_dict[company.strip()] = {
                    "reason": reason.strip(),
                    "count": 1,
                    "score": -1
                }
        
        for item in current_data.get("akjae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                company = company.strip()
                if company in akjae_dict:
                    akjae_dict[company]["count"] += 1
                    akjae_dict[company]["score"] -= 1
                    akjae_dict[company]["reason"] = reason.strip()
                else:
                    akjae_dict[company] = {
                        "reason": reason.strip(),
                        "count": 1,
                        "score": -1
                    }
        
        for company, data in akjae_dict.items():
            merged["akjae"].append(f"{company}: {data['reason']} ({data['count']}íšŒ)")
        
        return merged
    
    def _fallback_briefing(self, *, error: str, source_date: str | None = None):
        base_sections = {"ì •ì¹˜": "", "ê²½ì œ/ê±°ì‹œ": "", "ê¸°ì—…/ì‚°ì—…": "", "ë¶€ë™ì‚°": "", "êµ­ì œ": ""}
        msg = "ë¸Œë¦¬í•‘ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        if source_date:
            msg = f"ì˜¤ëŠ˜ ë¸Œë¦¬í•‘ ìƒì„±ì— ì‹¤íŒ¨í•˜ì—¬ ìµœê·¼ ìºì‹œ({source_date})ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
        base_sections["ê²½ì œ/ê±°ì‹œ"] = msg
        return {
            "section_summaries": base_sections,
            "hojae": [],
            "akjae": [],
            "trading_hojae": [],
            "trading_akjae": [],
            "analysis_mode": None,
            "is_holiday_next_day": False,
            "meta": {
                "generated_by": "fallback",
                "error": error,
                "source_date": source_date,
                "generated_at": datetime.now().isoformat(),
            },
        }

    def analyze_sentiment(self, categorized_news, date_str=None, *, use_cache=True, allow_stale=True, max_retries: int = 3):
        """ë¸Œë¦¬í•‘ + í˜¸ì¬/ì•…ì¬(ë° íŠ¸ë ˆì´ë”©ìš©) ìƒì„±.

        - use_cache=True: sentiment_cache/sentiment_YYYYMMDD.jsonì´ ìˆìœ¼ë©´ Gemini í˜¸ì¶œ ì—†ì´ ì¬ì‚¬ìš©
        - allow_stale=True: ë‹¹ì¼ ìºì‹œê°€ ì—†ê±°ë‚˜ Gemini ì‹¤íŒ¨ ì‹œ ìµœê·¼ ìºì‹œë¥¼ ëŒ€ì‹  í‘œì‹œ(ë¸Œë¦¬í•‘ ì„¹ì…˜ì´ ì‚¬ë¼ì§€ì§€ ì•Šë„ë¡)
        - max_retries: 503/UNAVAILABLE ë“±ì— ëŒ€í•´ exponential backoff ì¬ì‹œë„
        """
        if not categorized_news:
            return self._fallback_briefing(error="no categorized_news")

        if date_str is None:
            date_str = datetime.now().strftime("%Y%m%d")

        if use_cache:
            cached = self.load_cached_data(date_str)
            if cached is not None:
                print(f"âœ… ê°ì„±/ë¸Œë¦¬í•‘ ìºì‹œ ì¬ì‚¬ìš©: {self.get_cache_filename(date_str)}")
                return cached

            # ìºì‹œ ì¬ì‚¬ìš© ëª¨ë“œì¸ë° ì˜¤ëŠ˜ ìºì‹œê°€ ì—†ìœ¼ë©´, Gemini í˜¸ì¶œ ì—†ì´ ìµœì‹  ìºì‹œë¥¼ í´ë°±ìœ¼ë¡œ ì‚¬ìš©
            if allow_stale:
                stale_date, stale_path = self._find_latest_cache_file(exclude_date_str=date_str)
                if stale_path:
                    stale = self.load_cached_data(stale_date)
                    if stale is not None:
                        stale.setdefault("meta", {})
                        stale["meta"].update({
                            "generated_by": "stale_cache",
                            "source_date": stale_date,
                            "generated_at": datetime.now().isoformat(),
                        })
                        # ë‹¤ìŒ ì‹¤í–‰ë¶€í„°ëŠ” ë‹¹ì¼ ìºì‹œë¡œ ë°”ë¡œ ë¡œë“œ ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥
                        self.save_cached_data(stale, date_str)
                        print(f"âœ… ì˜¤ëŠ˜ ìºì‹œê°€ ì—†ì–´ ìµœê·¼ ë¸Œë¦¬í•‘ ìºì‹œ({stale_date})ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                        return stale

        # í˜„ì¬ ëª¨ë“œ í™•ì¸
        current_mode = self.is_trading_day()
        is_first_day_after_holiday = self.is_first_trading_day_after_holiday()
        print(f"ğŸ“Š í˜„ì¬ ëª¨ë“œ: {'í‰ì¼ ì¥ ì¤‘' if current_mode == 'trading' else 'ì£¼ë§/ê³µíœ´ì¼ ëˆ„ì '}")
        if is_first_day_after_holiday:
            print("ğŸ“… íœ´ì¼ ë‹¤ìŒ ë‚  ëª¨ë“œ: í†µí•© ì‹œê·¸ë„ ì ìš©")

        # ë¸Œë¦¬í•‘ìš© ì „ì²´ ë‰´ìŠ¤
        briefing_context = ""
        for category, items in categorized_news.items():
            if items:
                briefing_context += f"\n[{category}]\n"
                for item in items[:30]:
                    briefing_context += f"- {item['title']}\n"

        # ë§¤ë§¤ë´‡ìš© í•„í„°ë§ëœ ë‰´ìŠ¤
        trading_news = self.filter_trading_signals(categorized_news)
        trading_context = ""
        for category, items in trading_news.items():
            if items:
                trading_context += f"\n[{category}]\n"
                for item in items[:30]:
                    weight = item.get('time_weight', 1.0)
                    trading_context += f"- [{weight}x] {item['title']}\n"

        briefing_prompt = f"""
ë‹¹ì‹ ì€ ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì‘ì„±ìì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¶„ë¥˜ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨íˆ ì½ê¸° ì¢‹ì€ ì•„ì¹¨ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

í•„ìˆ˜ ê·œì¹™:
1. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
2. ê° ì„¹ì…˜ì˜ ìš”ì•½ì€ ìˆì—ˆë˜ ì‚¬ì‹¤ê³¼ ë¶„ìœ„ê¸°ë§Œ ì „ë‹¬í•˜ê³ , ì „ë§ì´ë‚˜ íŒë‹¨ì€ í•˜ì§€ ë§ˆì„¸ìš”.
3. ê°€ëŠ¥í•˜ë©´ ê¸ì •ì  íë¦„ê³¼ ë¶€ì •ì  ì´ìŠˆë¥¼ í•¨ê»˜ ë‹´ë˜ ê³¼ë„í•œ ì—°ê²° ì—†ì´ ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ìˆ˜í–‰ ê³¼ì œ:
1. **ëª¨ë‹ë¸Œë¦¬í•‘**: ë³¸ê²©ì ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ì½ê¸°ì „ì— ì˜¤ëŠ˜ì˜ ê³µê¸°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ ë¸Œë¦¬í•‘ì´ë¯€ë¡œ ìµœëŒ€í•œ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„±í•˜ì„¸ìš”.
2. **ê¸°ì—… ê°ì„± ë¶„ì„**: ì£¼ê°€ì— 'ì‹¤ì§ˆì 'ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ê²°ì •ì ì¸ í˜¸ì¬(Hojae)ì™€ ì•…ì¬(Akjae)ë¥¼ ì°¾ìœ¼ì„¸ìš”.
   - í˜¸ì¬ ì„ ì •: ëŒ€ê·œëª¨ ìˆ˜ì£¼(ìˆ˜ë°±ì–µ ì› ì´ìƒ), M&A, í•µì‹¬ ê¸°ìˆ  í˜ì‹ , ì‹¤ì  í„´ì–´ë¼ìš´ë“œ (ë‹¨ìˆœ ì¸ì‚¬ë‚˜ ì†Œê·œëª¨ í˜‘ì•½ì€ ì œì™¸).
   - ì•…ì¬ ì„ ì •: ì–´ë‹ ì‡¼í¬, ë²•ì  ë¶„ìŸ, ëŒ€ê·œëª¨ ë¦¬ì½œ, ìê¸ˆ ìœ ë™ì„± ìœ„ê¸°, ì£¼ìš” ìƒì‚° ì‹œì„¤ ì‚¬ê³ .
   - ì´ìœ  í‘œê¸°: ê° ê¸°ì—… ì˜†ì— 10ì ì´ë‚´ì˜ ì•„ì£¼ ì§§ì€ ì‚¬ìœ ë¥¼ ë§ë¶™ì´ì„¸ìš”.
   - í˜•ì‹: "íšŒì‚¬ëª…: ì‚¬ìœ "

Output JSON Format:
{{
  "section_summaries": {{
    "ì •ì¹˜": "...",
    "ê²½ì œ/ê±°ì‹œ": "...",
    "ê¸°ì—…/ì‚°ì—…": "...",
    "ë¶€ë™ì‚°": "...",
    "êµ­ì œ": "..."
  }},
  "hojae": ["íšŒì‚¬ëª…: ì‚¬ìœ "],
  "akjae": ["íšŒì‚¬ëª…: ì‚¬ìœ "]
}}

News List:
{briefing_context}
"""

        trading_prompt = f"""
ë‹¹ì‹ ì€ ë§¤ë§¤ë´‡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‰´ìŠ¤ ì¤‘ì—ì„œ ì˜¤ëŠ˜ì˜ ë§¤ë§¤ì— ì‹¤ì§ˆì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” í˜¸ì¬/ì•…ì¬ë§Œ ì„ ë³„í•´ì£¼ì„¸ìš”.

ì¤‘ìš” ê·œì¹™:
1. [2.0x] í‘œì‹œëœ ì¥ì™¸ ë‰´ìŠ¤(ì–´ì œ 15:30~ì˜¤ëŠ˜ 08:30)ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.
2. [1.0x] ì¥ì¤‘ ë‰´ìŠ¤ëŠ” ì´ë¯¸ ì£¼ê°€ì— ë°˜ì˜ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì‹ ì¤‘í•˜ê²Œ íŒë‹¨í•˜ì„¸ìš”.
3. ì˜¤ëŠ˜ì˜ ë§¤ë§¤ ì „ëµì— í˜¼ì„ ì„ ì¤„ ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ëŠ” ì œì™¸í•˜ì„¸ìš”.

ì„ ë³„ ê¸°ì¤€:
- í˜¸ì¬: ì¥ì™¸ ì‹œê°„ì— ë°œìƒí•œ ëŒ€ê·œëª¨ ìˆ˜ì£¼, M&A, ì‹¤ì  í„´ì–´ë¼ìš´ë“œ, í•µì‹¬ ê¸°ìˆ  ë‰´ìŠ¤
- ì•…ì¬: ì¥ì™¸ ì‹œê°„ì— ë°œìƒí•œ ì–´ë‹ ì‡¼í¬, ë²•ì  ë¶„ìŸ, ë¦¬ì½œ, ìœ ë™ì„± ìœ„ê¸°

Output JSON Format:
{{
  "trading_hojae": ["íšŒì‚¬ëª…: ì‚¬ìœ "],
  "trading_akjae": ["íšŒì‚¬ëª…: ì‚¬ìœ "]
}}

Weighted News List:
{trading_context}
"""

        try:
            briefing_data = self._generate_json_with_retry(
                briefing_prompt,
                model=config.model_flash,
                max_retries=max_retries,
            )
            trading_data = self._generate_json_with_retry(
                trading_prompt,
                model=config.model_flash,
                max_retries=max_retries,
            )

            final_data = {
                **briefing_data,
                "trading_hojae": trading_data.get("trading_hojae", []),
                "trading_akjae": trading_data.get("trading_akjae", []),
                "analysis_mode": current_mode,
                "is_holiday_next_day": is_first_day_after_holiday,
                "meta": {
                    "generated_by": "gemini",
                    "generated_at": datetime.now().isoformat(),
                },
            }

            # íœ´ì¼ ë‹¤ìŒ ë‚ ì´ë©´ ìºì‹œ ë³‘í•© ë° í†µí•© ë¦¬í¬íŠ¸
            if is_first_day_after_holiday:
                print("ğŸ”„ íœ´ì¼ ë‹¤ìŒ ë‚ : ìºì‹œ ë°ì´í„° í†µí•© ì¤‘...")
                final_data = self._merge_holiday_cache(final_data)
                self._clear_holiday_cache()

            # ëˆ„ì  ëª¨ë“œì¼ ê²½ìš° ìºì‹œ ë³‘í•©
            if current_mode == 'accumulation':
                cached_data = self.load_cached_data(date_str)
                if cached_data:
                    print("ğŸ”„ ìºì‹œëœ ë°ì´í„°ì™€ ë³‘í•© ì¤‘...")
                    final_data = self.merge_sentiment_data(final_data, cached_data)

            # ì„±ê³µ ì‹œì—ëŠ” ëª¨ë“œì™€ ë¬´ê´€í•˜ê²Œ ë‹¹ì¼ ìºì‹œ ì €ì¥(ì¬ì‚¬ìš© ì‹œê°„ëŒ€ì— Gemini í˜¸ì¶œ ë°©ì§€)
            if use_cache:
                self.save_cached_data(final_data, date_str)

            return final_data

        except Exception as e:
            print(f"Error generating sentiment analysis: {e}")

            # stale cache fallback
            if allow_stale:
                stale_date, stale_path = self._find_latest_cache_file(exclude_date_str=date_str)
                if stale_path:
                    stale = self.load_cached_data(stale_date)
                    if stale is not None:
                        stale.setdefault("meta", {})
                        stale["meta"].update({
                            "generated_by": "stale_cache",
                            "error": str(e),
                            "source_date": stale_date,
                            "generated_at": datetime.now().isoformat(),
                        })
                        # ì €ì¥í•´ë‘ë©´ ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ë‹¹ì¼ ìºì‹œë¡œ ë°”ë¡œ ë¡œë“œ ê°€ëŠ¥
                        if use_cache:
                            self.save_cached_data(stale, date_str)
                        return stale

            return self._fallback_briefing(error=str(e))
    
    def _merge_holiday_cache(self, current_data):
        """
        íœ´ì¼ ë‹¤ìŒ ë‚ ì— ìºì‹œëœ ëª¨ë“  ë°ì´í„°ë¥¼ ë³‘í•©
        """
        now = datetime.now()
        merged_data = {
            **current_data,
            "hojae": [],
            "akjae": [],
            "holiday_merged_dates": []
        }
        
        # ìµœê·¼ 3ì¼ê°„ì˜ ìºì‹œ ë°ì´í„° ìˆ˜ì§‘
        for i in range(1, 4):  # 1ì¼ì „, 2ì¼ì „, 3ì¼ì „
            past_date = now - timedelta(days=i)
            date_str = past_date.strftime("%Y%m%d")
            cached_data = self.load_cached_data(date_str)
            
            if cached_data:
                merged_data["holiday_merged_dates"].append(date_str)
                
                # í˜¸ì¬ ë°ì´í„° ë³‘í•©
                for item in cached_data.get("hojae", []):
                    if item not in merged_data["hojae"]:
                        merged_data["hojae"].append(item)
                
                # ì•…ì¬ ë°ì´í„° ë³‘í•©
                for item in cached_data.get("akjae", []):
                    if item not in merged_data["akjae"]:
                        merged_data["akjae"].append(item)
        
        # í˜„ì¬ ë°ì´í„° ì¶”ê°€
        merged_data["hojae"].extend(current_data.get("hojae", []))
        merged_data["akjae"].extend(current_data.get("akjae", []))
        
        # ì¤‘ë³µ ì œê±°
        merged_data["hojae"] = list(set(merged_data["hojae"]))
        merged_data["akjae"] = list(set(merged_data["akjae"]))
        
        print(f"âœ… íœ´ì¼ ë°ì´í„° í†µí•© ì™„ë£Œ: {len(merged_data['holiday_merged_dates'])}ì¼ê°„ ë°ì´í„° ë³‘í•©")
        
        return merged_data
    
    def _clear_holiday_cache(self):
        """
        íœ´ì¼ ìºì‹œ ì •ë¦¬
        """
        now = datetime.now()
        
        # ìµœê·¼ 3ì¼ê°„ ìºì‹œ íŒŒì¼ ì •ë¦¬
        for i in range(1, 4):
            past_date = now - timedelta(days=i)
            date_str = past_date.strftime("%Y%m%d")
            cache_file = self.get_cache_filename(date_str)
            
            if os.path.exists(cache_file):
                try:
                    os.remove(cache_file)
                    print(f"ğŸ—‘ï¸ ìºì‹œ íŒŒì¼ ì •ë¦¬: {cache_file}")
                except Exception as e:
                    print(f"ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def generate_weekend_summary(self):
        """
        ì£¼ë§ì— ìŒ“ì¸ ë°ì´í„°ë¥¼ ì›”ìš”ì¼ ì•„ì¹¨ì— í•œ ë²ˆì— ì¶œë ¥
        Returns: ì£¼ë§ ì¢…í•© ìš”ì•½ ë°ì´í„°
        """
        print("ğŸ“… ì£¼ë§ ë°ì´í„° ì¢…í•© ë¶„ì„ ì‹œì‘...")
        
        # ì£¼ë§ ë‚ ì§œ ê³„ì‚° (ê¸ˆìš”ì¼ë¶€í„° ì¼ìš”ì¼ê¹Œì§€)
        now = datetime.now()
        
        # ì›”ìš”ì¼ì¸ì§€ í™•ì¸
        if now.weekday() != 0:  # 0ì€ ì›”ìš”ì¼
            print("âš ï¸ ì›”ìš”ì¼ì´ ì•„ë‹ˆì–´ì„œ ì£¼ë§ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì£¼ë§ ë‚ ì§œë“¤ ê³„ì‚°
        friday = now - timedelta(days=3)
        saturday = now - timedelta(days=2) 
        sunday = now - timedelta(days=1)
        
        weekend_dates = [
            friday.strftime("%Y%m%d"),
            saturday.strftime("%Y%m%d"),
            sunday.strftime("%Y%m%d")
        ]
        
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ì£¼ë§: {friday.strftime('%m/%d')}~{sunday.strftime('%m/%d')}")
        
        # ì£¼ë§ ë°ì´í„° ìˆ˜ì§‘
        weekend_data = []
        for date_str in weekend_dates:
            cached_data = self.load_cached_data(date_str)
            if cached_data:
                cached_data["date"] = date_str
                weekend_data.append(cached_data)
        
        if not weekend_data:
            print("ğŸ“­ ì£¼ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì£¼ë§ ë°ì´í„° ì¢…í•© ë¶„ì„
        summary = self._analyze_weekend_data(weekend_data)
        
        return summary
    
    def _analyze_weekend_data(self, weekend_data):
        """
        ì£¼ë§ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìš”ì•½ ìƒì„±
        """
        print("ğŸ” ì£¼ë§ ë°ì´í„° ì¢…í•© ë¶„ì„ ì¤‘...")
        
        # ëª¨ë“  í˜¸ì¬/ì•…ì¬ ë°ì´í„° ìˆ˜ì§‘
        all_hojae = {}
        all_akjae = {}
        
        for day_data in weekend_data:
            date_str = day_data.get("date", "")
            
            # í˜¸ì¬ ë°ì´í„° ì²˜ë¦¬
            for item in day_data.get("hojae", []):
                if ":" in item:
                    company, reason = item.split(":", 1)
                    company = company.strip()
                    
                    if company not in all_hojae:
                        all_hojae[company] = {
                            "reasons": [],
                            "dates": [],
                            "count": 0
                        }
                    
                    all_hojae[company]["reasons"].append(reason.strip())
                    all_hojae[company]["dates"].append(date_str)
                    all_hojae[company]["count"] += 1
            
            # ì•…ì¬ ë°ì´í„° ì²˜ë¦¬
            for item in day_data.get("akjae", []):
                if ":" in item:
                    company, reason = item.split(":", 1)
                    company = company.strip()
                    
                    if company not in all_akjae:
                        all_akjae[company] = {
                            "reasons": [],
                            "dates": [],
                            "count": 0
                        }
                    
                    all_akjae[company]["reasons"].append(reason.strip())
                    all_akjae[company]["dates"].append(date_str)
                    all_akjae[company]["count"] += 1
        
        # ì£¼ë§ ìš”ì•½ ìƒì„±
        summary = {
            "weekend_dates": [data.get("date", "") for data in weekend_data],
            "hojae_summary": [],
            "akjae_summary": [],
            "top_hojae": [],
            "top_akjae": [],
            "total_hojae": len(all_hojae),
            "total_akjae": len(all_akjae)
        }
        
        # í˜¸ì¬ ìš”ì•½ (íšŒì‚¬ë³„ë¡œ ê·¸ë£¹í™”)
        for company, data in sorted(all_hojae.items(), key=lambda x: x[1]["count"], reverse=True):
            summary["hojae_summary"].append({
                "company": company,
                "count": data["count"],
                "reasons": data["reasons"],
                "dates": data["dates"]
            })
        
        # ì•…ì¬ ìš”ì•½
        for company, data in sorted(all_akjae.items(), key=lambda x: x[1]["count"], reverse=True):
            summary["akjae_summary"].append({
                "company": company,
                "count": data["count"],
                "reasons": data["reasons"],
                "dates": data["dates"]
            })
        
        # ìƒìœ„ í˜¸ì¬/ì•…ì¬ (3íšŒ ì´ìƒ ì–¸ê¸‰ëœ ê¸°ì—…)
        summary["top_hojae"] = [item for item in summary["hojae_summary"] if item["count"] >= 3]
        summary["top_akjae"] = [item for item in summary["akjae_summary"] if item["count"] >= 3]
        
        print(f"âœ… ì£¼ë§ ìš”ì•½ ì™„ë£Œ: í˜¸ì¬ {len(all_hojae)}ê°œ, ì•…ì¬ {len(all_akjae)}ê°œ")
        
        return summary
    
    def format_weekend_summary_message(self, weekend_summary):
        """
        ì£¼ë§ ìš”ì•½ì„ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ë¡œ í¬ë§·íŒ…
        """
        if not weekend_summary:
            return None
        
        dates = weekend_summary.get("weekend_dates", [])
        if dates:
            date_range = f"{dates[-1][-4:]}-{dates[-1][4:6]}-{dates[-1][6:8]} ~ {dates[0][-4:]}-{dates[0][4:6]}-{dates[0][6:8]}"
        else:
            date_range = "ì£¼ë§"
        
        lines = [
            f"ğŸ“Š ì£¼ë§ ì¢…í•© ë‰´ìŠ¤ ìš”ì•½ ({date_range})",
            "",
            f"ğŸ“ˆ í˜¸ì¬ ê¸°ì—…: {weekend_summary.get('total_hojae', 0)}ê°œ",
            f"ğŸ“‰ ì•…ì¬ ê¸°ì—…: {weekend_summary.get('total_akjae', 0)}ê°œ",
            ""
        ]
        
        # ìƒìœ„ í˜¸ì¬ ê¸°ì—…
        top_hojae = weekend_summary.get("top_hojae", [])
        if top_hojae:
            lines.append("ğŸ”¥ ì£¼ìš” í˜¸ì¬ ê¸°ì—…:")
            for item in top_hojae[:5]:  # ìƒìœ„ 5ê°œ
                lines.append(f"  â€¢ {item['company']}: {item['count']}íšŒ")
            lines.append("")
        
        # ìƒìœ„ ì•…ì¬ ê¸°ì—…
        top_akjae = weekend_summary.get("top_akjae", [])
        if top_akjae:
            lines.append("âš ï¸ ì£¼ìš” ì•…ì¬ ê¸°ì—…:")
            for item in top_akjae[:5]:  # ìƒìœ„ 5ê°œ
                lines.append(f"  â€¢ {item['company']}: {item['count']}íšŒ")
            lines.append("")
        
        lines.append("ğŸ“± ìì„¸í•œ ë‚´ìš©ì€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”!")
        
        return "\n".join(lines)

    # NOTE: ê³¼ê±°ì— analyze_sentiment()ê°€ íŒŒì¼ ë‚´ì— 2ë²ˆ ì •ì˜ë˜ì–´ ì•„ë˜ìª½ì´ ìœ„ ë¡œì§ì„ ë®ì–´ì“°ë˜ ë¬¸ì œê°€ ìˆì—ˆìŒ.
    # í˜„ì¬ëŠ” ìœ„ì˜ analyze_sentiment() í•˜ë‚˜ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.

    def extract_hojae_list(self, briefing_data):
        """
        Extract hojae list from briefing data.
        Returns a list of hojae items.
        """
        if not briefing_data:
            return []
        
        return briefing_data.get("hojae", [])

    def extract_akjae_list(self, briefing_data):
        """
        Extract akjae list from briefing data.
        Returns a list of akjae items.
        """
        if not briefing_data:
            return []
        
        return briefing_data.get("akjae", [])

    def get_hojae_count(self, briefing_data):
        """
        Get the count of hojae items.
        """
        return len(self.extract_hojae_list(briefing_data))

    def get_akjae_count(self, briefing_data):
        """
        Get the count of akjae items.
        """
        return len(self.extract_akjae_list(briefing_data))

    def format_telegram_message(self, briefing_data, date_str=None, total_news_count=None):
        """
        Format telegram message for hojae notification.
        Returns a formatted message string.
        """
        hojae_list = self.extract_hojae_list(briefing_data)
        hojae_count = len(hojae_list)
        
        if not hojae_list:
            return None
        
        # Compose summary header (date, total news, hojae count)
        headline = "ğŸ“° ëª¨ë‹ë‰´ìŠ¤ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤"
        if date_str:
            headline += f" ({date_str})"

        summary_parts = []
        if total_news_count is not None:
            summary_parts.append(f"ì´ {total_news_count}ê±´ì˜ ë‰´ìŠ¤")
        summary_parts.append(f"í˜¸ì¬ ê¸°ì—…: {hojae_count}ê³³")
        summary_line = " / ".join(summary_parts)

        list_title = f"ğŸ“ˆ í˜¸ì¬ ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ({date_str})" if date_str else "ğŸ“ˆ í˜¸ì¬ ê¸°ì—… ë¦¬ìŠ¤íŠ¸"

        lines = [headline, summary_line, "", list_title]
        for item in hojae_list:
            lines.append(f"- {item}")
        message = "\n".join(lines)
        
        return message

    def get_section_summaries(self, briefing_data):
        """
        Extract section summaries from briefing data.
        """
        if not briefing_data:
            return {}
        
        return briefing_data.get("section_summaries", {})

    def has_sentiment_data(self, briefing_data):
        """
        Check if briefing data contains sentiment analysis (hojae/akjae).
        """
        if not briefing_data:
            return False
        
        return bool(briefing_data.get("hojae") or briefing_data.get("akjae"))

if __name__ == "__main__":
    # Test the SentimentAnalyzer
    analyzer = SentimentAnalyzer()
    
    print("ğŸ§ª SentimentAnalyzer í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ 1: ë‚ ì§œ íŒë‹¨ ë¡œì§
    print(f"ğŸ“… í˜„ì¬ ëª¨ë“œ: {analyzer.is_trading_day()}")
    
    # í…ŒìŠ¤íŠ¸ 2: ìºì‹œ ê¸°ëŠ¥
    test_data = {
        "hojae": ["ì‚¼ì„±ì „ì: ë°˜ë„ì²´ í˜¸ì¬", "LGí™”í•™: ë°°í„°ë¦¬ í˜¸ì¬"],
        "akjae": ["í˜„ëŒ€ì°¨: ë¦¬ì½œ ì•…ì¬"],
        "section_summaries": {"ì •ì¹˜": "í…ŒìŠ¤íŠ¸ ìš”ì•½"}
    }
    
    analyzer.save_cached_data(test_data, "20260117")
    loaded_data = analyzer.load_cached_data("20260117")
    print(f"âœ… ìºì‹œ í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if loaded_data else 'ì‹¤íŒ¨'}")
    
    # í…ŒìŠ¤íŠ¸ 3: ë°ì´í„° ë³‘í•©
    current_data = {
        "hojae": ["ì‚¼ì„±ì „ì: ì‹ ì œí’ˆ ì¶œì‹œ", "SKí•˜ì´ë‹‰ìŠ¤: ìˆ˜ì£¼"],
        "akjae": ["í˜„ëŒ€ì°¨: ë¦¬ì½œ ì•…ì¬", "ê¸°ì•„: ë¶€í’ˆ ê²°í•¨"]
    }
    
    merged = analyzer.merge_sentiment_data(current_data, test_data)
    print(f"âœ… ë³‘í•© í…ŒìŠ¤íŠ¸: í˜¸ì¬ {len(merged['hojae'])}ê°œ, ì•…ì¬ {len(merged['akjae'])}ê°œ")
    
    # í…ŒìŠ¤íŠ¸ 4: ì£¼ë§ ìš”ì•½ (ì›”ìš”ì¼ì´ ì•„ë‹ˆë©´ None ë°˜í™˜)
    weekend_summary = analyzer.generate_weekend_summary()
    print(f"ğŸ“… ì£¼ë§ ìš”ì•½ í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if weekend_summary else 'ì›”ìš”ì¼ì´ ì•„ë‹˜'}")
    
    print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
