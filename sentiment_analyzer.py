from google import genai
from openai import OpenAI
import json
import re
import os
import time
import glob
import random
from datetime import datetime, timedelta, timezone
from config import config

class SentimentAnalyzer:
    def __init__(self):
        self.client = genai.Client(api_key=config.gemini_api_key)
        self.openai_client = OpenAI(api_key=config.openai_api_key) if config.openai_api_key else None
        self.cache_dir = "sentiment_cache"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.tts_dir = "scripts"
        os.makedirs(self.tts_dir, exist_ok=True)
        self.brief_dir = "scripts"
        os.makedirs(self.brief_dir, exist_ok=True)

    @staticmethod
    def _kst_now():
        return datetime.now(timezone(timedelta(hours=9)))

    @staticmethod
    def _format_korean_date(date_obj: datetime) -> str:
        return f"{date_obj.month}ì›” {date_obj.day}ì¼"

    # -----------------------------
    # Cache helpers
    # -----------------------------
        
    def is_trading_day(self):
        """
        í˜„ì¬ê°€ í‰ì¼ ì¥ ì¤‘ì¸ì§€, ì•„ë‹ˆë©´ ì£¼ë§/ê³µíœ´ì¼ì¸ì§€ íŒë‹¨
        Returns: 'trading' (í‰ì¼ ì¥ ì¤‘) or 'accumulation' (ì£¼ë§/ê³µíœ´ì¼)
        """
        now = datetime.now()
        
        # ì£¼ë§ ì²´í¬
        if now.weekday() >= 5:  # í† ìš”ì¼(5), ì¼ìš”ì¼(6)
            return 'accumulation'
        
        # ê³µíœ´ì¼ ì²´í¬ (ê°„ë‹¨í•œ í•œêµ­ ê³µíœ´ì¼ ëª©ë¡)
        month = now.month
        day = now.day
        
        korean_holidays = {
            (1, 1): "ì‹ ì •",
            (3, 1): "ì‚¼ì¼ì ˆ",
            (5, 5): "ì–´ë¦°ì´ë‚ ",
            (6, 6): "í˜„ì¶©ì¼",
            (8, 15): "ê´‘ë³µì ˆ",
            (10, 3): "ê°œì²œì ˆ",
            (10, 9): "í•œê¸€ë‚ ",
            (12, 25): "ì„±íƒ„ì ˆ"
        }
        
        if (month, day) in korean_holidays:
            return 'accumulation'
        
        # ì¥ ì‹œê°„ ì²´í¬ (í‰ì¼ 9:00-15:30)
        if 9 <= now.hour < 16:
            return 'trading'
        else:
            return 'accumulation'
    
    def is_first_trading_day_after_holiday(self):
        """
        ì›”ìš”ì¼ ë˜ëŠ” ê³µíœ´ì¼ ë‹¤ìŒ ë‚ ì¸ì§€ íŒë‹¨
        Returns: True if today is the first trading day after a holiday/weekend
        """
        now = datetime.now()
        
        # ì›”ìš”ì¼ ì²´í¬
        if now.weekday() == 0:  # 0ì€ ì›”ìš”ì¼
            return True
        
        # ì–´ì œê°€ ê³µíœ´ì¼ì´ì—ˆëŠ”ì§€ ì²´í¬
        yesterday = now - timedelta(days=1)
        yesterday_month = yesterday.month
        yesterday_day = yesterday.day
        
        korean_holidays = {
            (1, 1): "ì‹ ì •",
            (3, 1): "ì‚¼ì¼ì ˆ",
            (5, 5): "ì–´ë¦°ì´ë‚ ",
            (6, 6): "í˜„ì¶©ì¼",
            (8, 15): "ê´‘ë³µì ˆ",
            (10, 3): "ê°œì²œì ˆ",
            (10, 9): "í•œê¸€ë‚ ",
            (12, 25): "ì„±íƒ„ì ˆ"
        }
        
        # ì–´ì œê°€ ì£¼ë§ì´ê±°ë‚˜ ê³µíœ´ì¼ì´ì—ˆìœ¼ë©´ True
        if yesterday.weekday() >= 5 or (yesterday_month, yesterday_day) in korean_holidays:
            return True
        
        return False
    
    def get_time_weight(self, news_datetime):
        """
        ë‰´ìŠ¤ ì‹œê°„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê³„ì‚°
        - ì–´ì œ 15:30 ~ ì˜¤ëŠ˜ 08:30: ê°€ì¤‘ì¹˜ 2.0 (ì¥ì™¸ ë‰´ìŠ¤)
        - ì–´ì œ 09:00 ~ 15:30: ê°€ì¤‘ì¹˜ 1.0 (ì¥ì¤‘ ë‰´ìŠ¤)
        - ê·¸ ì™¸: ê°€ì¤‘ì¹˜ 0.5 (ê¸°íƒ€)
        """
        now = datetime.now()
        yesterday = now - timedelta(days=1)
        
        # ì¥ì™¸ ì‹œê°„ëŒ€: ì–´ì œ 15:30 ~ ì˜¤ëŠ˜ 08:30
        market_close_yesterday = yesterday.replace(hour=15, minute=30, second=0, microsecond=0)
        market_open_today = now.replace(hour=8, minute=30, second=0, microsecond=0)
        
        # ì¥ì¤‘ ì‹œê°„ëŒ€: ì–´ì œ 09:00 ~ 15:30
        market_open_yesterday = yesterday.replace(hour=9, minute=0, second=0, microsecond=0)
        
        if market_close_yesterday <= news_datetime <= market_open_today:
            return 2.0  # ì¥ì™¸ ë‰´ìŠ¤ (ê°€ì¥ ì¤‘ìš”)
        elif market_open_yesterday <= news_datetime < market_close_yesterday:
            return 1.0  # ì¥ì¤‘ ë‰´ìŠ¤ (ì´ë¯¸ ë°˜ì˜ë¨)
        else:
            return 0.5  # ê¸°íƒ€ ë‰´ìŠ¤
    
    def filter_trading_signals(self, categorized_news):
        """
        ë§¤ë§¤ë´‡ìš© í˜¸ì¬/ì•…ì¬ í•„í„°ë§
        - ì¥ì™¸ ë‰´ìŠ¤(ì–´ì œ 15:30~ì˜¤ëŠ˜ 08:30)ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        """
        filtered_news = {}
        
        for category, items in categorized_news.items():
            filtered_items = []
            
            for item in items:
                news_time = item.get('published_dt')
                if news_time:
                    weight = self.get_time_weight(news_time)
                    # ê°€ì¤‘ì¹˜ê°€ 1.0 ì´ìƒì¸ ë‰´ìŠ¤ë§Œ í¬í•¨ (ì¥ì™¸ + ì¥ì¤‘)
                    if weight >= 1.0:
                        item['time_weight'] = weight
                        filtered_items.append(item)
            
            if filtered_items:
                filtered_news[category] = filtered_items
        
        return filtered_news
    
    def get_cache_filename(self, date_str=None):
        """
        ìºì‹œ íŒŒì¼ëª… ìƒì„±
        """
        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")
        return os.path.join(self.cache_dir, f"sentiment_{date_str}.json")

    def _find_latest_cache_file(self, exclude_date_str=None, max_age_days: int | None = 2):
        """Find newest sentiment_YYYYMMDD.json.

        Args:
            exclude_date_str: íŠ¹ì • ë‚ ì§œ(YYYYMMDD)ëŠ” ì œì™¸
            max_age_days: ì˜¤ëŠ˜ ê¸°ì¤€ ìµœëŒ€ ëª‡ ì¼ ì „ê¹Œì§€ í—ˆìš©í• ì§€. Noneì´ë©´ ì œí•œ ì—†ìŒ.

        Returns:
            (date_str, path) or (None, None)
        """
        candidates = sorted(glob.glob(os.path.join(self.cache_dir, "sentiment_*.json")))
        # Sort by filename date then mtime as tie-breaker
        def _key(p):
            base = os.path.basename(p)
            m = base.replace("sentiment_", "").replace(".json", "")
            try:
                dt = datetime.strptime(m, "%Y%m%d")
            except Exception:
                dt = datetime.fromtimestamp(os.path.getmtime(p))
            return (dt, os.path.getmtime(p))
        candidates.sort(key=_key, reverse=True)

        now = datetime.now()
        for path in candidates:
            base = os.path.basename(path)
            date_str = base.replace("sentiment_", "").replace(".json", "")
            if exclude_date_str and date_str == exclude_date_str:
                continue

            if max_age_days is not None:
                try:
                    dt = datetime.strptime(date_str, "%Y%m%d")
                    if (now - dt).days > max_age_days:
                        continue
                except Exception:
                    # If we can't parse date, skip it when age limiting is enabled
                    continue

            return date_str, path
        return None, None
    
    def load_cached_data(self, date_str=None):
        """
        ìºì‹œëœ ë°ì´í„° ë¡œë“œ
        """
        cache_file = self.get_cache_filename(date_str)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def save_cached_data(self, data, date_str=None):
        """
        ë°ì´í„° ìºì‹œ ì €ì¥
        """
        cache_file = self.get_cache_filename(date_str)
        tmp_file = f"{cache_file}.tmp"
        try:
            with open(tmp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            os.replace(tmp_file, cache_file)
            print(f"âœ… ê°ì„± ë°ì´í„° ìºì‹œ ì €ì¥: {cache_file}")
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # -----------------------------
    # Retry helpers
    # -----------------------------
    @staticmethod
    def _is_retryable_error(err: Exception) -> bool:
        msg = str(err)
        # Gemini/Google GenAI commonly returns "503 UNAVAILABLE" when overloaded.
        return (
            "503" in msg
            or "UNAVAILABLE" in msg
            or "RESOURCE_EXHAUSTED" in msg
            or "429" in msg
            or "overloaded" in msg.lower()
            or "rate" in msg.lower() and "limit" in msg.lower()
        )

    def _generate_json_with_retry(self, prompt: str, *, model: str, max_retries: int = 3, base_sleep_sec: float = 3.0):
        last_err = None
        for attempt in range(1, max_retries + 1):
            try:
                resp = self.client.models.generate_content(
                    model=model,
                    contents=prompt,
                    config={'response_mime_type': 'application/json'}
                )
                return json.loads(resp.text)
            except Exception as e:
                last_err = e
                if attempt < max_retries and self._is_retryable_error(e):
                    jitter = random.uniform(0.0, base_sleep_sec * 0.6)
                    sleep_sec = base_sleep_sec * (2 ** (attempt - 1)) + jitter
                    print(f"âš ï¸ Gemini í˜¸ì¶œ ì‹¤íŒ¨(ì¬ì‹œë„ {attempt}/{max_retries}): {e}")
                    print(f"   -> {sleep_sec:.1f}s í›„ ì¬ì‹œë„")
                    time.sleep(sleep_sec)
                    continue
                raise

        # Should never reach here
        raise last_err

    def _generate_text_with_openai(self, prompt: str, *, model: str, max_retries: int = 3, base_sleep_sec: float = 3.0) -> str:
        if not self.openai_client:
            raise RuntimeError("OPENAI_API_KEY is not configured")

        last_err = None
        for attempt in range(1, max_retries + 1):
            try:
                resp = self.openai_client.responses.create(
                    model=model,
                    input=[
                        {
                            "role": "system",
                            "content": "You output plain text only. Do not include JSON, markdown, or commentary.",
                        },
                        {
                            "role": "user",
                            "content": prompt,
                        },
                    ],
                )
                payload = resp.output_text or ""
                return payload.strip()
            except Exception as e:
                last_err = e
                if attempt < max_retries and self._is_retryable_error(e):
                    jitter = random.uniform(0.0, base_sleep_sec * 0.6)
                    sleep_sec = base_sleep_sec * (2 ** (attempt - 1)) + jitter
                    print(f"âš ï¸ OpenAI í˜¸ì¶œ ì‹¤íŒ¨(ì¬ì‹œë„ {attempt}/{max_retries}): {e}")
                    print(f"   -> {sleep_sec:.1f}s í›„ ì¬ì‹œë„")
                    time.sleep(sleep_sec)
                    continue
                raise

        raise last_err

    def _generate_tts_script_with_openai(self, prompt: str, *, model: str, max_retries: int = 3, base_sleep_sec: float = 3.0) -> dict:
        """Generate anchor script using OpenAI chat.completions API and return JSON.

        Args:
            prompt: The prompt to send to OpenAI
            model: The model to use (e.g., "gpt-4o")
            max_retries: Maximum number of retry attempts
            base_sleep_sec: Base sleep duration for exponential backoff

        Returns:
            dict: Parsed JSON response containing source_script only
        """
        if not self.openai_client:
            raise RuntimeError("OPENAI_API_KEY is not configured")

        last_err = None
        for attempt in range(1, max_retries + 1):
            try:
                resp = self.openai_client.chat.completions.create(
                    model=model,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are a Korean news briefing script writer. Output ONLY valid JSON, no markdown, no commentary, no code blocks.",
                        },
                        {
                            "role": "user",
                            "content": prompt,
                        },
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.7,
                )
                
                content = resp.choices[0].message.content
                if not content:
                    raise ValueError("OpenAI returned empty content")
                
                # Parse JSON response
                try:
                    result = json.loads(content)
                    # Expect source_script to be plain text (string)
                    source_script = result.get("source_script") if isinstance(result, dict) else None
                    if not isinstance(source_script, str) or not source_script.strip():
                        raise ValueError("OpenAI response missing valid source_script text")
                    return result
                except json.JSONDecodeError as je:
                    print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {je}")
                    print(f"   ì‘ë‹µ ë‚´ìš©: {content[:200]}...")
                    raise ValueError(f"Invalid JSON response from OpenAI: {je}")
                    
            except Exception as e:
                last_err = e
                if attempt < max_retries and self._is_retryable_error(e):
                    jitter = random.uniform(0.0, base_sleep_sec * 0.6)
                    sleep_sec = base_sleep_sec * (2 ** (attempt - 1)) + jitter
                    print(f"âš ï¸ OpenAI í˜¸ì¶œ ì‹¤íŒ¨(ì¬ì‹œë„ {attempt}/{max_retries}): {e}")
                    print(f"   -> {sleep_sec:.1f}s í›„ ì¬ì‹œë„")
                    time.sleep(sleep_sec)
                    continue
                raise

        raise last_err
    
    def merge_sentiment_data(self, current_data, cached_data):
        """
        í˜„ì¬ ë°ì´í„°ì™€ ìºì‹œëœ ë°ì´í„°ë¥¼ ë³‘í•©
        - ë™ì¼ ì¢…ëª©ì€ ì ìˆ˜ í•©ì‚°
        - ìƒˆë¡œìš´ ì¢…ëª©ì€ ì¶”ê°€
        """
        if not cached_data:
            return current_data
        
        merged = {
            "section_summaries": cached_data.get("section_summaries", {}),
            "hojae": [],
            "akjae": [],
            "merged_dates": cached_data.get("merged_dates", [])
        }
        
        # í˜„ì¬ ë‚ ì§œ ì¶”ê°€
        today = datetime.now().strftime("%Y-%m-%d")
        if today not in merged["merged_dates"]:
            merged["merged_dates"].append(today)
        
        # í˜¸ì¬ ë°ì´í„° ë³‘í•©
        hojae_dict = {}
        
        # ìºì‹œëœ í˜¸ì¬ ë°ì´í„° ì²˜ë¦¬
        for item in cached_data.get("hojae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                hojae_dict[company.strip()] = {
                    "reason": reason.strip(),
                    "count": 1,
                    "score": 1
                }
        
        # í˜„ì¬ í˜¸ì¬ ë°ì´í„° ì²˜ë¦¬ ë° ë³‘í•©
        for item in current_data.get("hojae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                company = company.strip()
                if company in hojae_dict:
                    hojae_dict[company]["count"] += 1
                    hojae_dict[company]["score"] += 1
                    # ìµœì‹  ì´ìœ ë¡œ ì—…ë°ì´íŠ¸
                    hojae_dict[company]["reason"] = reason.strip()
                else:
                    hojae_dict[company] = {
                        "reason": reason.strip(),
                        "count": 1,
                        "score": 1
                    }
        
        # ë³‘í•©ëœ í˜¸ì¬ ë°ì´í„° ìƒì„±
        for company, data in hojae_dict.items():
            merged["hojae"].append(f"{company}: {data['reason']} ({data['count']}íšŒ)")
        
        # ì•…ì¬ ë°ì´í„° ë³‘í•© (ë™ì¼ ë¡œì§)
        akjae_dict = {}
        
        for item in cached_data.get("akjae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                akjae_dict[company.strip()] = {
                    "reason": reason.strip(),
                    "count": 1,
                    "score": -1
                }
        
        for item in current_data.get("akjae", []):
            if ":" in item:
                company, reason = item.split(":", 1)
                company = company.strip()
                if company in akjae_dict:
                    akjae_dict[company]["count"] += 1
                    akjae_dict[company]["score"] -= 1
                    akjae_dict[company]["reason"] = reason.strip()
                else:
                    akjae_dict[company] = {
                        "reason": reason.strip(),
                        "count": 1,
                        "score": -1
                    }
        
        for company, data in akjae_dict.items():
            merged["akjae"].append(f"{company}: {data['reason']} ({data['count']}íšŒ)")
        
        return merged
    
    def _fallback_briefing(self, *, error: str, source_date: str | None = None):
        base_sections = {"ì •ì¹˜": "", "ê²½ì œ/ê±°ì‹œ": "", "ê¸°ì—…/ì‚°ì—…": "", "ë¶€ë™ì‚°": "", "êµ­ì œ": ""}
        msg = "ë¸Œë¦¬í•‘ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        if source_date:
            msg = f"ì˜¤ëŠ˜ ë¸Œë¦¬í•‘ ìƒì„±ì— ì‹¤íŒ¨í•˜ì—¬ ìµœê·¼ ìºì‹œ({source_date})ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
        base_sections["ê²½ì œ/ê±°ì‹œ"] = msg
        return {
            "section_summaries": base_sections,
            "hojae": [],
            "akjae": [],
            "analysis_mode": None,
            "is_holiday_next_day": False,
            "meta": {
                "generated_by": "fallback",
                "error": error,
                "source_date": source_date,
                "generated_at": datetime.now().isoformat(),
            },
        }

    def _ensure_tts_script(self, data: dict, date_str: str | None = None) -> dict:
        """Ensure tts_script exists and is well-formed for backward compatibility."""
        if data is None:
            return data
        if not isinstance(data, dict):
            print(f"âš ï¸ _ensure_tts_script received non-dict: {type(data)}")
            return {
                "section_summaries": {"ì •ì¹˜": "", "ê²½ì œ/ê±°ì‹œ": "", "ê¸°ì—…/ì‚°ì—…": "", "ë¶€ë™ì‚°": "", "êµ­ì œ": ""},
                "hojae": [],
                "akjae": [],
                "tts_script": {},
                "meta": {
                    "generated_by": "fallback",
                    "error": f"non-dict briefing data: {type(data)}",
                    "generated_at": datetime.now().isoformat(),
                },
            }

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")
        date_str_dash = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"

        tts_script = data.get("tts_script")
        if not isinstance(tts_script, dict):
            tts_script = {}

        tts_script.setdefault("duration_sec_target", 300)
        tts_script.setdefault("title", f"ì˜¤ëŠ˜ì˜ ëª¨ë‹ë‰´ìŠ¤ ({date_str_dash})")

        pronunciations = tts_script.get("pronunciations", [])
        if not isinstance(pronunciations, list):
            pronunciations = []
        tts_script["pronunciations"] = pronunciations

        lines = tts_script.get("lines", [])
        if isinstance(lines, str):
            lines = [lines]
        if not isinstance(lines, list):
            lines = []
        tts_script["lines"] = [str(line).strip() for line in lines if str(line).strip()]

        data["tts_script"] = tts_script
        return data

    def _build_tts_fallback_content(self, briefing_data: dict, date_str: str | None = None) -> list[str]:
        """Build a raw, parser-friendly script when tts_script is missing."""
        if briefing_data is None:
            return []

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")
        date_str_dash = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"

        section_summaries = briefing_data.get("section_summaries", {}) or {}
        hojae = briefing_data.get("hojae", []) or []
        akjae = briefing_data.get("akjae", []) or []

        lines: list[str] = [
            f"Title: ì˜¤ëŠ˜ì˜ ëª¨ë‹ë‰´ìŠ¤ ({date_str_dash})",
            f"Date: {date_str_dash}",
            "Sections:",
        ]

        for key in ["ì •ì¹˜", "ê²½ì œ/ê±°ì‹œ", "ê¸°ì—…/ì‚°ì—…", "ë¶€ë™ì‚°", "êµ­ì œ"]:
            summary = section_summaries.get(key, "").strip() if isinstance(section_summaries, dict) else ""
            lines.append(f"- {key}: {summary}")

        lines.append("Hojae:")
        if hojae:
            lines.extend([f"- {item}" for item in hojae])
        else:
            lines.append("- (none)")

        lines.append("Akjae:")
        if akjae:
            lines.extend([f"- {item}" for item in akjae])
        else:
            lines.append("- (none)")

        return lines

    def _build_tts_lines_from_source_script(self, source_script: dict | str | None) -> list[str]:
        """Flatten OpenAI source_script into a list of TTS lines.

        source_script is the master content. It can be either a plain text string
        (new format) or a structured dict (legacy format). This function flattens
        it and splits long sentences for better TTS readability.
        """
        if source_script is None:
            return []

        lines: list[str] = []

        def _split_long_line(text: str) -> list[str]:
            if not text:
                return []
            parts = re.split(r"(?<=[.!?ã€‚ï¼Ÿï¼])\s+|,\s+|Â·\s+|ê·¸ë¦¬ê³ \s+|í•˜ì§€ë§Œ\s+", text)
            cleaned = [p.strip() for p in parts if p and p.strip()]
            return cleaned if cleaned else [text.strip()]

        def _extend(part):
            if isinstance(part, list):
                for item in part:
                    text = str(item).strip()
                    if not text:
                        continue
                    lines.extend(_split_long_line(text))
            elif isinstance(part, str):
                value = part.strip()
                if value:
                    lines.extend(_split_long_line(value))

        if isinstance(source_script, str):
            paragraphs = [p.strip() for p in re.split(r"\n\s*\n", source_script) if p.strip()]
            for paragraph in paragraphs:
                sentence_chunks = re.split(r"(?<=[.!?ã€‚ï¼Ÿï¼])\s+", paragraph)
                for chunk in sentence_chunks:
                    text = chunk.strip()
                    if not text:
                        continue
                    lines.extend(_split_long_line(text))
            return [line for line in lines if line]

        if not isinstance(source_script, dict):
            return []

        _extend(source_script.get("intro"))

        sections = source_script.get("sections", {})
        if not isinstance(sections, dict):
            sections = {}
        for key in ["ì •ì¹˜", "ê²½ì œ/ê±°ì‹œ", "ê¸°ì—…/ì‚°ì—…", "ë¶€ë™ì‚°", "êµ­ì œ"]:
            _extend(sections.get(key))

        positive = source_script.get("positive", {})
        if isinstance(positive, dict):
            _extend(positive.get("theme"))
            _extend(positive.get("items"))

        negative = source_script.get("negative", {})
        if isinstance(negative, dict):
            _extend(negative.get("theme"))
            _extend(negative.get("items"))

        _extend(source_script.get("outro"))

        return [line for line in lines if line]

    def _pad_tts_lines(self, lines: list[str], target_min: int = 55, target_max: int = 75) -> list[str]:
        """Ensure TTS lines count falls within target range by splitting/padding."""
        if not lines:
            return lines
        # ìµœì†Œ ì¤„ ìˆ˜ ê°•ì œëŠ” ë¹„í™œì„±í™”: ë‰´ìŠ¤ê°€ ì ì€ ë‚ ì€ ì§§ê²Œ í—ˆìš©
        if len(lines) > target_max:
            lines = lines[: target_max - 1] + [lines[-1]]

        return lines

    def _ensure_tts_outro(self, lines: list[str]) -> list[str]:
        """Force the last TTS line to the fixed outro."""
        if not lines:
            return lines
        outro = "ì˜¤ëŠ˜ ë‰´ìŠ¤ ìš”ì•½ì€ ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤. ë‚´ì¼ ì•„ì¹¨ì— ë˜ ë§Œë‚˜ìš”."
        if lines[-1] != outro:
            lines[-1] = outro
        return lines

    def save_tts_script_text(self, briefing_data: dict, date_str: str | None = None) -> str | None:
        """Save TTS script to a text file and return its path."""
        if briefing_data is None:
            return None

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")

        briefing_data = self._ensure_tts_script(briefing_data, date_str)
        tts_script = briefing_data.get("tts_script") or {}
        lines = tts_script.get("lines", [])
        if isinstance(lines, str):
            lines = [lines]
        if not isinstance(lines, list):
            lines = []

        brief_scripts = briefing_data.get("brief_scripts") if isinstance(briefing_data, dict) else None
        if isinstance(brief_scripts, dict):
            source_script = brief_scripts.get("source_script")
            regenerated = self._build_tts_lines_from_source_script(source_script)
            if regenerated:
                regenerated = self._pad_tts_lines(regenerated)
                regenerated = self._ensure_tts_outro(regenerated)
                lines = regenerated

        lines = [str(line).strip() for line in lines if str(line).strip()]

        filename = os.path.join(self.tts_dir, f"youtube_tts_{date_str}.txt")
        tmp_file = f"{filename}.tmp"

        pronunciations = tts_script.get("pronunciations", [])
        pronunciation_lines = []
        for item in pronunciations:
            term = item.get("term") if isinstance(item, dict) else None
            say = item.get("say") if isinstance(item, dict) else None
            if term and say:
                pronunciation_lines.append(f"- {term} -> {say}")

        content_parts = [
            f"Title: {tts_script.get('title', '')}",
            f"DurationSecTarget: {tts_script.get('duration_sec_target', 300)}",
        ]
        if pronunciation_lines:
            content_parts.append("Pronunciations:")
            content_parts.extend(pronunciation_lines)

        if lines:
            content_parts.append("TtsLines:")
            content_parts.extend(lines)
        else:
            content_parts.append("RawScript:")
            content_parts.extend(self._build_tts_fallback_content(briefing_data, date_str))

        content = "\n".join(content_parts).strip() + "\n"

        try:
            os.makedirs(self.tts_dir, exist_ok=True)
            with open(tmp_file, "w", encoding="utf-8") as f:
                f.write(content)
            os.replace(tmp_file, filename)
            return filename
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"âš ï¸ TTS ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def save_brief_scripts_json(self, brief_scripts: dict | None, date_str: str | None = None) -> str | None:
        """Save source/read scripts to a single JSON file and return its path."""
        if not brief_scripts or not isinstance(brief_scripts, dict):
            return None

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")

        filename = os.path.join(self.brief_dir, f"brief_{date_str}.json")
        tmp_file = f"{filename}.tmp"
        payload = {
            "source_script": brief_scripts.get("source_script"),

        }

        try:
            os.makedirs(self.brief_dir, exist_ok=True)
            with open(tmp_file, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
            os.replace(tmp_file, filename)
            return filename
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"âš ï¸ ë¸Œë¦¬í”„ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def save_srt_script_json(self, source_script: dict | None, date_str: str | None = None) -> str | None:
        """Save source_script JSON for SRT use."""
        if not source_script or not isinstance(source_script, dict):
            return None

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")

        filename = os.path.join(self.brief_dir, f"srt_{date_str}.json")
        tmp_file = f"{filename}.tmp"

        try:
            os.makedirs(self.brief_dir, exist_ok=True)
            with open(tmp_file, "w", encoding="utf-8") as f:
                json.dump(source_script, f, ensure_ascii=False, indent=2)
            os.replace(tmp_file, filename)
            return filename
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"âš ï¸ SRT ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def save_tts_script_json(self, source_script: dict | None, date_str: str | None = None) -> str | None:
        """Save source_script JSON for reference (deprecated, kept for backward compatibility)."""
        if not source_script or not isinstance(source_script, dict):
            return None

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")

        filename = os.path.join(self.brief_dir, f"tts_{date_str}.json")
        tmp_file = f"{filename}.tmp"

        try:
            os.makedirs(self.brief_dir, exist_ok=True)
            with open(tmp_file, "w", encoding="utf-8") as f:
                json.dump(source_script, f, ensure_ascii=False, indent=2)
            os.replace(tmp_file, filename)
            return filename
        except Exception as e:
            if os.path.exists(tmp_file):
                try:
                    os.remove(tmp_file)
                except Exception:
                    pass
            print(f"âš ï¸ TTS JSON ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def _build_brief_prompt_from_context(self, context_lines: list[str]) -> str:
        """Build OpenAI brief prompt with escaped braces."""
        template = """
ë‹¹ì‹ ì€ **í•œêµ­ì–´ ë¼ë””ì˜¤ ëª¨ë‹ë‰´ìŠ¤ ì•µì»¤**ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ê¸°ì‚¬ ìš”ì•½ì´ ì•„ë‹ˆë¼,
ì¶œê·¼ê¸¸ì— ë¼ë””ì˜¤ë¥¼ í‹€ì–´ë‘” ì²­ì·¨ìê°€ ìì—°ìŠ¤ëŸ½ê²Œ ëê¹Œì§€ ë“¤ì„ ìˆ˜ ìˆëŠ”
5ë¶„ ë‚´ì™¸ì˜ ì•„ì¹¨ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì›ê³ ë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì•„ë˜ ì…ë ¥ì€ ì´ë¯¸ ì„ ë³„Â·ìš”ì•½ëœ ë‰´ìŠ¤ ë°ì´í„°ì…ë‹ˆë‹¤.
ì´ ì…ë ¥ë§Œ ì‚¬ìš©í•´ ì›ê³ ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ì ˆëŒ€ ê·œì¹™:
- ê¸°ì‚¬ ëª©ë¡ì²˜ëŸ¼ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”.
- ì„¹ì…˜ ì œëª©ì„ ì“°ì§€ ë§ˆì„¸ìš”. (ì •ì¹˜, ê²½ì œ ê°™ì€ ë§ ê¸ˆì§€)
- ë¶ˆë¦¿, ë²ˆí˜¸, ëª©ë¡í˜• ë¬¸ì¥ ê¸ˆì§€
- â€œ~ì…ë‹ˆë‹¤, ~í–ˆìŠµë‹ˆë‹¤â€ í†¤ì˜ ë‚­ë…ì²´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- í•œ ë¬¸ì¥ì€ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ, ë§ë¡œ ì½íˆëŠ” ë¦¬ë“¬ì„ ìœ ì§€í•˜ì„¸ìš”.
- ê°™ì€ ë‰´ìŠ¤ ì†Œì¬ë¥¼ ë‘ ë²ˆ ì´ìƒ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.

ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  êµ¬ì„± íë¦„:
1. ì˜¤í”„ë‹: ì§§ì€ ì¸ì‚¬ + ì˜¤ëŠ˜ ë‰´ìŠ¤ì˜ í° íë¦„ ì˜ˆê³ 
2. êµ­ë‚´ ì£¼ìš” ì´ìŠˆ: ì •ì¹˜Â·ì •ì±…Â·í–‰ì • ì´ìŠˆë¥¼ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì—°ê²° (ì‚¬ì‹¤ â†’ ë§¥ë½ â†’ í˜„ì¬ ë¶„ìœ„ê¸°)
3. ê²½ì œÂ·ìƒí™œ ì²´ê° ë‰´ìŠ¤: ë¬¼ê°€, ì¦ì‹œ, ìƒí™œê³¼ ì—°ê²°ë˜ëŠ” ì´ìŠˆ (ì²´ê°/ë¶„ìœ„ê¸°/ë°˜ì‘ í‘œí˜„ í™œìš©)
4. ì‚°ì—…Â·ê¸°ìˆ  íë¦„: ë°˜ë„ì²´, ë°©ì‚°, AI ë“±ì€ ê²½ìŸ êµ¬ë„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª… (ê¸°ì—…ëª… ìµœì†Œí™”)
5. ë¶€ë™ì‚°Â·ì£¼ê±° ì´ìŠˆ: ê°€ê²© ë‚˜ì—´ ê¸ˆì§€, ì‹œì¥ ì‹¬ë¦¬/ì²´ê° ì¤‘ì‹¬
6. êµ­ì œÂ·ì•ˆë³´ ì´ìŠˆ: ì§€ì—­ë³„ ë¬¶ì–´ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° (ê¸´ì¥/ë³€í™”/ê´€ì¸¡ í‘œí˜„ í™œìš©)
7. í´ë¡œì§•: ì˜¤ëŠ˜ ë‰´ìŠ¤ í•µì‹¬ í•œ ë¬¸ì¥ ì •ë¦¬ + ë‚´ì¼ ë‹¤ì‹œ ë§Œë‚œë‹¤ëŠ” ë§

ë¬¸ì¥ ì‘ì„± ê°€ì´ë“œ:
- â€œ~ë¡œ ë³´ì…ë‹ˆë‹¤â€
- â€œ~ë¼ëŠ” ë°˜ì‘ë„ ë‚˜ì˜µë‹ˆë‹¤â€
- â€œë¶„ìœ„ê¸°ê°€ ì´ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤â€
- â€œì²´ê°ìƒ ë³€í™”ê°€ í¬ì§€ ì•Šë‹¤ëŠ” ë§ë„ ë‚˜ì˜µë‹ˆë‹¤â€
- â€œí•´ì„ì€ ì—‡ê°ˆë¦½ë‹ˆë‹¤â€

íŒ©íŠ¸ + í•´ì„ 1ì¤„ì€ í—ˆìš©ë©ë‹ˆë‹¤.
ì˜ê²¬Â·í‰ê°€Â·ì˜ˆì¸¡ ì¶”ê°€ëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹:
- í•˜ë‚˜ì˜ ì—°ì†ëœ ì›ê³ ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì¤„ë°”ê¿ˆì€ ë¬¸ë‹¨ êµ¬ë¶„ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì„¹ì…˜ ì œëª©, ì†Œì œëª©, êµ¬ë¶„ì ì‚¬ìš© ê¸ˆì§€

Output JSON (JSON í˜•ì‹ë§Œ í—ˆìš©):
{{
  "source_script": "ì›ê³  ì „ì²´ í…ìŠ¤íŠ¸"
}}

ì…ë ¥ ë°ì´í„°:
{input_data}
"""
        return template.format(input_data="\n".join(context_lines))

    def ensure_brief_scripts(self, briefing_data: dict, date_str: str | None = None, *, max_retries: int = 3) -> dict:
        """Ensure brief_scripts are generated using existing briefing data."""
        if briefing_data is None:
            raise ValueError("briefing_data is required")

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")

        section_summaries = briefing_data.get("section_summaries", {}) or {}
        hojae = briefing_data.get("hojae", []) or []
        akjae = briefing_data.get("akjae", []) or []

        context_lines = ["ì„¹ì…˜ ìš”ì•½:"]
        for key in ["ì •ì¹˜", "ê²½ì œ/ê±°ì‹œ", "ê¸°ì—…/ì‚°ì—…", "ë¶€ë™ì‚°", "êµ­ì œ"]:
            summary = section_summaries.get(key, "") if isinstance(section_summaries, dict) else ""
            context_lines.append(f"- {key}: {summary}")
        if hojae:
            context_lines.append("í˜¸ì¬:")
            context_lines.extend([f"- {item}" for item in hojae])
        if akjae:
            context_lines.append("ì•…ì¬:")
            context_lines.extend([f"- {item}" for item in akjae])

        brief_prompt = self._build_brief_prompt_from_context(context_lines)

        brief_data = self._generate_tts_script_with_openai(
            brief_prompt,
            model=config.openai_model_tts,
            max_retries=max_retries,
        )
        if isinstance(brief_data, dict) and isinstance(brief_data.get("source_script"), str):
            briefing_data = {**briefing_data}
            briefing_data["brief_scripts"] = {
                "source_script": brief_data.get("source_script"),
            }
            briefing_data.setdefault("meta", {})["brief_generated_by"] = "openai"
            return briefing_data

        raise ValueError("OpenAI brief script generation returned unexpected payload")

    def regenerate_tts_only(self, briefing_data: dict, date_str: str | None = None, *, max_retries: int = 3) -> dict:
        if briefing_data is None:
            raise ValueError("briefing_data is required for TTS-only regeneration")

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")

        date_str_dash = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        kst_korean_date = self._format_korean_date(self._kst_now())

        section_summaries = briefing_data.get("section_summaries", {}) or {}
        hojae = briefing_data.get("hojae", []) or []
        akjae = briefing_data.get("akjae", []) or []

        context_lines = ["ì„¹ì…˜ ìš”ì•½:"]
        for key in ["ì •ì¹˜", "ê²½ì œ/ê±°ì‹œ", "ê¸°ì—…/ì‚°ì—…", "ë¶€ë™ì‚°", "êµ­ì œ"]:
            summary = section_summaries.get(key, "") if isinstance(section_summaries, dict) else ""
            context_lines.append(f"- {key}: {summary}")
        if hojae:
            context_lines.append("í˜¸ì¬:")
            context_lines.extend([f"- {item}" for item in hojae])
        if akjae:
            context_lines.append("ì•…ì¬:")
            context_lines.extend([f"- {item}" for item in akjae])

        brief_prompt = self._build_brief_prompt_from_context(context_lines)

        brief_data = self._generate_tts_script_with_openai(
            brief_prompt,
            model=config.openai_model_tts,
            max_retries=max_retries,
        )
        if isinstance(brief_data, dict) and isinstance(brief_data.get("source_script"), str):
            briefing_data = {**briefing_data}
            briefing_data["brief_scripts"] = {
                "source_script": brief_data.get("source_script"),
            }
            briefing_data.setdefault("meta", {})["brief_generated_by"] = "openai"
            return briefing_data

        raise ValueError("OpenAI brief script generation returned unexpected payload")

    def analyze_sentiment(self, categorized_news, date_str=None, *, use_cache=True, allow_stale=True, max_retries: int = 3):
        """ë¸Œë¦¬í•‘ + í˜¸ì¬/ì•…ì¬ ìƒì„±.

        - use_cache=True: sentiment_cache/sentiment_YYYYMMDD.jsonì´ ìˆìœ¼ë©´ Gemini í˜¸ì¶œ ì—†ì´ ì¬ì‚¬ìš©
        - allow_stale=True: ë‹¹ì¼ ìºì‹œê°€ ì—†ê±°ë‚˜ Gemini ì‹¤íŒ¨ ì‹œ ìµœê·¼ ìºì‹œë¥¼ ëŒ€ì‹  í‘œì‹œ(ë¸Œë¦¬í•‘ ì„¹ì…˜ì´ ì‚¬ë¼ì§€ì§€ ì•Šë„ë¡)
        - max_retries: 503/UNAVAILABLE ë“±ì— ëŒ€í•´ exponential backoff ì¬ì‹œë„
        """
        if not categorized_news:
            return self._fallback_briefing(error="no categorized_news")

        if date_str is None:
            date_str = self._kst_now().strftime("%Y%m%d")

        if use_cache:
            cached = self.load_cached_data(date_str)
            if cached is not None:
                print(f"âœ… ê°ì„±/ë¸Œë¦¬í•‘ ìºì‹œ ì¬ì‚¬ìš©: {self.get_cache_filename(date_str)}")
                cached = self._ensure_tts_script(cached, date_str)
                brief_scripts_cached = cached.get("brief_scripts")
                if isinstance(brief_scripts_cached, dict):
                    openai_tts_lines = self._build_tts_lines_from_source_script(
                        brief_scripts_cached.get("source_script")
                    )
                    if openai_tts_lines:
                        openai_tts_lines = self._pad_tts_lines(openai_tts_lines)
                        openai_tts_lines = self._ensure_tts_outro(openai_tts_lines)
                        cached.setdefault("tts_script", {})
                        cached["tts_script"]["lines"] = openai_tts_lines
                        cached.setdefault("meta", {})["tts_lines_generated_by"] = "openai_source_script"
                        if isinstance(cached.get("meta"), dict):
                            cached["meta"].pop("tts_fallback", None)
                        cached = self._normalize_tts_script(cached, date_str)
                        if use_cache:
                            self.save_cached_data(cached, date_str)
                return cached

            # ìºì‹œ ì¬ì‚¬ìš© ëª¨ë“œì¸ë° ì˜¤ëŠ˜ ìºì‹œê°€ ì—†ìœ¼ë©´, Gemini í˜¸ì¶œ ì—†ì´ ìµœì‹  ìºì‹œë¥¼ í´ë°±ìœ¼ë¡œ ì‚¬ìš©
            if allow_stale:
                stale_date, stale_path = self._find_latest_cache_file(exclude_date_str=date_str)
                if stale_path:
                    stale = self.load_cached_data(stale_date)
                    if stale is not None:
                        stale.setdefault("meta", {})
                        stale["meta"].update({
                            "generated_by": "stale_cache",
                            "source_date": stale_date,
                            "generated_at": datetime.now().isoformat(),
                        })
                        stale = self._ensure_tts_script(stale, date_str)
                        # ë‹¤ìŒ ì‹¤í–‰ë¶€í„°ëŠ” ë‹¹ì¼ ìºì‹œë¡œ ë°”ë¡œ ë¡œë“œ ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥
                        self.save_cached_data(stale, date_str)
                        print(f"âœ… ì˜¤ëŠ˜ ìºì‹œê°€ ì—†ì–´ ìµœê·¼ ë¸Œë¦¬í•‘ ìºì‹œ({stale_date})ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                        return stale

        # í˜„ì¬ ëª¨ë“œ í™•ì¸
        current_mode = self.is_trading_day()
        is_first_day_after_holiday = self.is_first_trading_day_after_holiday()
        print(f"ğŸ“Š í˜„ì¬ ëª¨ë“œ: {'í‰ì¼ ì¥ ì¤‘' if current_mode == 'trading' else 'ì£¼ë§/ê³µíœ´ì¼ ëˆ„ì '}")
        if is_first_day_after_holiday:
            print("ğŸ“… íœ´ì¼ ë‹¤ìŒ ë‚  ëª¨ë“œ: í†µí•© ì‹œê·¸ë„ ì ìš©")

        # ë¸Œë¦¬í•‘ìš© ì „ì²´ ë‰´ìŠ¤
        briefing_context = ""
        for category, items in categorized_news.items():
            if items:
                briefing_context += f"\n[{category}]\n"
                for item in items[:30]:
                    briefing_context += f"- {item['title']}\n"

        briefing_json_schema = """
{
  "section_summaries": {
    "ì •ì¹˜": "...",
    "ê²½ì œ/ê±°ì‹œ": "...",
    "ê¸°ì—…/ì‚°ì—…": "...",
    "ë¶€ë™ì‚°": "...",
    "êµ­ì œ": "..."
  },
  "hojae": ["íšŒì‚¬ëª…: ì‚¬ìœ "],
  "akjae": ["íšŒì‚¬ëª…: ì‚¬ìœ "],
}
""".strip()

        kst_now = self._kst_now()
        date_str_dash = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        kst_korean_date = self._format_korean_date(kst_now)

        briefing_prompt = f"""
ë‹¹ì‹ ì€ ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì‘ì„±ìì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¶„ë¥˜ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨íˆ ì½ê¸° ì¢‹ì€ ì•„ì¹¨ ë¸Œë¦¬í•‘ì„ ì‘ì„±í•˜ì„¸ìš”.

í•„ìˆ˜ ê·œì¹™:
1. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
2. ê° ì„¹ì…˜ì˜ ìš”ì•½ì€ ìˆì—ˆë˜ ì‚¬ì‹¤ê³¼ ë¶„ìœ„ê¸°ë§Œ ì „ë‹¬í•˜ê³ , ì „ë§ì´ë‚˜ íŒë‹¨ì€ í•˜ì§€ ë§ˆì„¸ìš”.
3. ê°€ëŠ¥í•˜ë©´ ê¸ì •ì  íë¦„ê³¼ ë¶€ì •ì  ì´ìŠˆë¥¼ í•¨ê»˜ ë‹´ë˜ ê³¼ë„í•œ ì—°ê²° ì—†ì´ ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ìˆ˜í–‰ ê³¼ì œ:
1. **ëª¨ë‹ë¸Œë¦¬í•‘**: ë³¸ê²©ì ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ì½ê¸°ì „ì— ì˜¤ëŠ˜ì˜ ê³µê¸°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ ë¸Œë¦¬í•‘ì´ë¯€ë¡œ ìµœëŒ€í•œ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„±í•˜ì„¸ìš”.
2. **ê¸°ì—… ê°ì„± ë¶„ì„**: ì£¼ê°€ì— 'ì‹¤ì§ˆì 'ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ê²°ì •ì ì¸ í˜¸ì¬(Hojae)ì™€ ì•…ì¬(Akjae)ë¥¼ ì°¾ìœ¼ì„¸ìš”.
   - í˜¸ì¬ ì„ ì •: ëŒ€ê·œëª¨ ìˆ˜ì£¼(ìˆ˜ë°±ì–µ ì› ì´ìƒ), M&A, í•µì‹¬ ê¸°ìˆ  í˜ì‹ , ì‹¤ì  í„´ì–´ë¼ìš´ë“œ (ë‹¨ìˆœ ì¸ì‚¬ë‚˜ ì†Œê·œëª¨ í˜‘ì•½ì€ ì œì™¸).
   - ì•…ì¬ ì„ ì •: ì–´ë‹ ì‡¼í¬, ë²•ì  ë¶„ìŸ, ëŒ€ê·œëª¨ ë¦¬ì½œ, ìê¸ˆ ìœ ë™ì„± ìœ„ê¸°, ì£¼ìš” ìƒì‚° ì‹œì„¤ ì‚¬ê³ .
   - ì´ìœ  í‘œê¸°: ê° ê¸°ì—… ì˜†ì— 10ì ì´ë‚´ì˜ ì•„ì£¼ ì§§ì€ ì‚¬ìœ ë¥¼ ë§ë¶™ì´ì„¸ìš”.
   - í˜•ì‹: "íšŒì‚¬ëª…: ì‚¬ìœ "

TTS ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ê·œì¹™:
1. 5ë¶„ ë‚­ë… ê¸°ì¤€(4ë¶„30ì´ˆ~5ë¶„30ì´ˆ)ìœ¼ë¡œ 55~75ì¤„ ë‚´ì™¸ ì‘ì„±.
2. ê¸°ì‚¬ì²´ ê¸ˆì§€. ë§í•˜ê¸°ì²´ë¡œ ì§§ì€ ë¬¸ì¥(í•œ ì¤„=í•œ ë¬¸ì¥).
3. ì„¹ì…˜ ì „í™˜ ë©˜íŠ¸ í¬í•¨: "ë‹¤ìŒì€â€¦", "í•œí¸â€¦", "ì •ë¦¬í•˜ë©´â€¦" ë“±.
4. ìˆ«ì/ì•½ì–´/ë‹¨ìœ„ëŠ” TTSê°€ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜í•˜ê±°ë‚˜ pronunciationsì— ë“±ë¡.
   - ì˜ˆ: "3.2%" â†’ "ì‚¼ì©œì´ í¼ì„¼íŠ¸", "1,200ë‹¬ëŸ¬" â†’ "ì²œì´ë°± ë‹¬ëŸ¬"
5. ë§ˆì§€ë§‰ ì¤„ì€ ë°˜ë“œì‹œ ê³ ì • ë¬¸êµ¬ ì‚¬ìš©:
   "ì˜¤ëŠ˜ ë‰´ìŠ¤ ìš”ì•½ì€ ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤. ë‚´ì¼ ì•„ì¹¨ì— ë˜ ë§Œë‚˜ìš”."
6. ì²« ì¤„ì€ ë°˜ë“œì‹œ ë‚ ì§œë¥¼ í¬í•¨í•´ ì•„ë˜ í˜•ì‹ì„ ì§€í‚¤ì„¸ìš”:
   "[SMILE] ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤. {kst_korean_date} ëª¨ë‹ë‰´ìŠ¤ ì‹œì‘í•©ë‹ˆë‹¤. [PAUSE 0.4]"
7. titleì€ ë°˜ë“œì‹œ "ì˜¤ëŠ˜ì˜ ëª¨ë‹ë‰´ìŠ¤ ({date_str_dash})"ë¡œ ì„¤ì •í•˜ì„¸ìš”.

Output JSON Format:
{briefing_json_schema}

News List:
{briefing_context}
"""

        try:
            briefing_data = self._generate_json_with_retry(
                briefing_prompt,
                model=config.model_flash,
                max_retries=max_retries,
            )

            if not isinstance(briefing_data, dict):
                raise ValueError(f"Unexpected briefing_data type: {type(briefing_data)}")

            base_sections = {"ì •ì¹˜": "", "ê²½ì œ/ê±°ì‹œ": "", "ê¸°ì—…/ì‚°ì—…": "", "ë¶€ë™ì‚°": "", "êµ­ì œ": ""}
            section_summaries = briefing_data.get("section_summaries")
            if not isinstance(section_summaries, dict):
                section_summaries = {}
            merged_sections = {**base_sections, **{k: v for k, v in section_summaries.items() if isinstance(k, str)}}
            briefing_data["section_summaries"] = merged_sections

            if not isinstance(briefing_data.get("hojae"), list):
                briefing_data["hojae"] = []
            if not isinstance(briefing_data.get("akjae"), list):
                briefing_data["akjae"] = []

            final_data = {
                **self._ensure_tts_script(briefing_data, date_str),
                "analysis_mode": current_mode,
                "is_holiday_next_day": is_first_day_after_holiday,
                "meta": {
                    "generated_by": "gemini",
                    "generated_at": datetime.now().isoformat(),
                },
            }

            brief_scripts_payload = None
            try:
                brief_context_lines = ["ì„¹ì…˜ ìš”ì•½:"]
                for key in ["ì •ì¹˜", "ê²½ì œ/ê±°ì‹œ", "ê¸°ì—…/ì‚°ì—…", "ë¶€ë™ì‚°", "êµ­ì œ"]:
                    summary = briefing_data.get("section_summaries", {}).get(key, "") if isinstance(briefing_data.get("section_summaries"), dict) else ""
                    brief_context_lines.append(f"- {key}: {summary}")
                if briefing_data.get("hojae"):
                    brief_context_lines.append("í˜¸ì¬:")
                    brief_context_lines.extend([f"- {item}" for item in briefing_data.get("hojae", [])])
                if briefing_data.get("akjae"):
                    brief_context_lines.append("ì•…ì¬:")
                    brief_context_lines.extend([f"- {item}" for item in briefing_data.get("akjae", [])])

                brief_prompt = self._build_brief_prompt_from_context(brief_context_lines)
                brief_data = self._generate_tts_script_with_openai(
                    brief_prompt,
                    model=config.openai_model_tts,
                    max_retries=max_retries,
                )
                if isinstance(brief_data, dict) and isinstance(brief_data.get("source_script"), str):
                    brief_scripts_payload = {
                        "source_script": brief_data.get("source_script"),
                    }
                    final_data["brief_scripts"] = brief_scripts_payload
                    final_data.setdefault("meta", {})["brief_generated_by"] = "openai"
                    openai_tts_lines = self._build_tts_lines_from_source_script(
                        brief_scripts_payload.get("source_script")
                    )
                    if openai_tts_lines:
                        openai_tts_lines = self._pad_tts_lines(openai_tts_lines)
                        openai_tts_lines = self._ensure_tts_outro(openai_tts_lines)
                        final_data.setdefault("tts_script", {})
                        final_data["tts_script"]["lines"] = openai_tts_lines
                        final_data.setdefault("meta", {})["tts_lines_generated_by"] = "openai_source_script"
                        if isinstance(final_data.get("meta"), dict):
                            final_data["meta"].pop("tts_fallback", None)
            except Exception as e:
                print(f"âš ï¸ OpenAI ë¸Œë¦¬í”„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                final_data.setdefault("meta", {})["brief_generated_by"] = "gemini_fallback"

            final_data = self._normalize_tts_script(final_data, date_str)
            if not self._validate_tts_script(final_data, date_str):
                print("âš ï¸ TTS ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦ ì‹¤íŒ¨: ì¬ìƒì„± ì‹œë„")
                retry_prompt = briefing_prompt + "\n\nì¤‘ìš”: ìœ„ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”. ë‚ ì§œ/ì¤„ ìˆ˜ë¥¼ ìœ„ë°˜í•˜ë©´ ì‹¤íŒ¨ì…ë‹ˆë‹¤."
                briefing_data = self._generate_json_with_retry(
                    retry_prompt,
                    model=config.model_flash,
                    max_retries=max_retries,
                )
                if not isinstance(briefing_data, dict):
                    raise ValueError(f"Unexpected briefing_data type after retry: {type(briefing_data)}")
                final_data = {
                    **self._ensure_tts_script(briefing_data, date_str),
                    "analysis_mode": current_mode,
                    "is_holiday_next_day": is_first_day_after_holiday,
                    "meta": {
                        "generated_by": "gemini_retry",
                        "generated_at": datetime.now().isoformat(),
                    },
                }
                if brief_scripts_payload:
                    final_data["brief_scripts"] = brief_scripts_payload
                    final_data.setdefault("meta", {})["brief_generated_by"] = "openai"
                    openai_tts_lines = self._build_tts_lines_from_source_script(
                        brief_scripts_payload.get("source_script")
                    )
                    if openai_tts_lines:
                        openai_tts_lines = self._pad_tts_lines(openai_tts_lines)
                        openai_tts_lines = self._ensure_tts_outro(openai_tts_lines)
                        final_data.setdefault("tts_script", {})
                        final_data["tts_script"]["lines"] = openai_tts_lines
                        final_data.setdefault("meta", {})["tts_lines_generated_by"] = "openai_source_script"
                        if isinstance(final_data.get("meta"), dict):
                            final_data["meta"].pop("tts_fallback", None)
                final_data = self._normalize_tts_script(final_data, date_str)

            if not self._validate_tts_script(final_data, date_str):
                print("âš ï¸ TTS ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦ ì‹¤íŒ¨: OpenAI ê¸°ë°˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ëŒ€ì²´")
                if brief_scripts_payload:
                    openai_tts_lines = self._build_tts_lines_from_source_script(
                        brief_scripts_payload.get("source_script")
                    )
                    if openai_tts_lines:
                        openai_tts_lines = self._pad_tts_lines(openai_tts_lines)
                        openai_tts_lines = self._ensure_tts_outro(openai_tts_lines)
                        final_data.setdefault("tts_script", {})
                        final_data["tts_script"]["lines"] = openai_tts_lines
                        final_data.setdefault("meta", {})["tts_lines_generated_by"] = "openai_source_script"
                        if isinstance(final_data.get("meta"), dict):
                            final_data["meta"].pop("tts_fallback", None)
                        final_data = self._normalize_tts_script(final_data, date_str)
                if not self._validate_tts_script(final_data, date_str):
                    print("âš ï¸ TTS ìŠ¤í¬ë¦½íŠ¸ ê²€ì¦ ì‹¤íŒ¨: í´ë°± ìŠ¤í¬ë¦½íŠ¸ë¡œ ëŒ€ì²´")
                    final_data = self._apply_tts_fallback(final_data, date_str)
                if brief_scripts_payload:
                    final_data["brief_scripts"] = brief_scripts_payload
                    final_data.setdefault("meta", {})["brief_generated_by"] = "openai"

            # íœ´ì¼ ë‹¤ìŒ ë‚ ì´ë©´ ìºì‹œ ë³‘í•© ë° í†µí•© ë¦¬í¬íŠ¸
            if is_first_day_after_holiday:
                print("ğŸ”„ íœ´ì¼ ë‹¤ìŒ ë‚ : ìºì‹œ ë°ì´í„° í†µí•© ì¤‘...")
                final_data = self._merge_holiday_cache(final_data)
                self._clear_holiday_cache()

            # ëˆ„ì  ëª¨ë“œì¼ ê²½ìš° ìºì‹œ ë³‘í•©
            if current_mode == 'accumulation':
                cached_data = self.load_cached_data(date_str)
                if cached_data:
                    print("ğŸ”„ ìºì‹œëœ ë°ì´í„°ì™€ ë³‘í•© ì¤‘...")
                    final_data = self.merge_sentiment_data(final_data, cached_data)

            # ì„±ê³µ ì‹œì—ëŠ” ëª¨ë“œì™€ ë¬´ê´€í•˜ê²Œ ë‹¹ì¼ ìºì‹œ ì €ì¥(ì¬ì‚¬ìš© ì‹œê°„ëŒ€ì— Gemini í˜¸ì¶œ ë°©ì§€)
            if use_cache:
                self.save_cached_data(final_data, date_str)

            return final_data

        except Exception as e:
            print(f"Error generating sentiment analysis: {e}")

            # stale cache fallback
            if allow_stale:
                stale_date, stale_path = self._find_latest_cache_file(exclude_date_str=date_str)
                if stale_path:
                    stale = self.load_cached_data(stale_date)
                    if stale is not None:
                        stale.setdefault("meta", {})
                        stale["meta"].update({
                            "generated_by": "stale_cache",
                            "error": str(e),
                            "source_date": stale_date,
                            "generated_at": datetime.now().isoformat(),
                        })
                        stale = self._ensure_tts_script(stale, date_str)
                        # ì €ì¥í•´ë‘ë©´ ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ë‹¹ì¼ ìºì‹œë¡œ ë°”ë¡œ ë¡œë“œ ê°€ëŠ¥
                        if use_cache:
                            self.save_cached_data(stale, date_str)
                        return stale

            return self._fallback_briefing(error=str(e))

    def _normalize_tts_script(self, data: dict, date_str: str) -> dict:
        if not data:
            return data

        kst_now = self._kst_now()
        date_str_dash = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        kst_korean_date = self._format_korean_date(kst_now)

        tts_script = data.get("tts_script") or {}
        tts_script["title"] = f"ì˜¤ëŠ˜ì˜ ëª¨ë‹ë‰´ìŠ¤ ({date_str_dash})"

        lines = tts_script.get("lines", [])
        if isinstance(lines, str):
            lines = [lines]
        if not isinstance(lines, list):
            lines = []

        intro = f"[SMILE] ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤. {kst_korean_date} ëª¨ë‹ë‰´ìŠ¤ ì‹œì‘í•©ë‹ˆë‹¤. [PAUSE 0.4]"
        if lines:
            lines[0] = intro
        else:
            lines = [intro]

        tts_script["lines"] = [str(line).strip() for line in lines if str(line).strip()]
        data["tts_script"] = tts_script
        return data

    def _validate_tts_script(self, data: dict, date_str: str) -> bool:
        if not data:
            return False

        tts_script = data.get("tts_script") or {}
        title = tts_script.get("title", "")
        lines = tts_script.get("lines", [])
        if isinstance(lines, str):
            lines = [lines]
        if not isinstance(lines, list):
            return False

        date_str_dash = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        kst_korean_date = self._format_korean_date(self._kst_now())

        if date_str_dash not in title:
            return False

        if not lines:
            return False

        if kst_korean_date not in lines[0]:
            return False

        line_count = len(lines)
        if line_count < 1:
            return False
        if line_count > 75:
            return False

        return True

    def _apply_tts_fallback(self, data: dict, date_str: str) -> dict:
        if not data:
            data = {}

        kst_now = self._kst_now()
        date_str_dash = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        kst_korean_date = self._format_korean_date(kst_now)

        intro = f"[SMILE] ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤. {kst_korean_date} ëª¨ë‹ë‰´ìŠ¤ ì‹œì‘í•©ë‹ˆë‹¤. [PAUSE 0.4]"
        outro = "ì˜¤ëŠ˜ ë‰´ìŠ¤ ìš”ì•½ì€ ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤. ë‚´ì¼ ì•„ì¹¨ì— ë˜ ë§Œë‚˜ìš”."

        tts_script = data.get("tts_script") or {}
        lines = tts_script.get("lines", [])
        if isinstance(lines, str):
            lines = [lines]
        if not isinstance(lines, list):
            lines = []

        core_lines = [line for line in lines[1:] if isinstance(line, str)]
        core_lines = [line for line in core_lines if line.strip() and line.strip() != outro]
        if not core_lines:
            core_lines = [
                "ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ íë¦„ì„ ì •ë¦¬í•´ ë“œë¦½ë‹ˆë‹¤.",
                "ì •ì¹˜ê¶Œì—ì„œëŠ” ì§€ì—­ ìˆœíšŒ ì¼ì •ê³¼ ì •ì±… ë…¼ì˜ê°€ ì´ì–´ì¡ŒìŠµë‹ˆë‹¤.",
                "ê²½ì œëŠ” ì™¸êµ­ì¸ ìˆ˜ê¸‰ê³¼ í™˜ìœ¨ ë³€ë™ì´ ë™ì‹œì— ì£¼ëª©ë°›ì•˜ìŠµë‹ˆë‹¤.",
                "ê¸°ì—… í˜„ì¥ì—ì„œëŠ” ë°˜ë„ì²´ íˆ¬ì í™•ëŒ€ì™€ ì‹¤ì  ë°œí‘œê°€ í™”ë‘ì˜€ìŠµë‹ˆë‹¤.",
                "ë¶€ë™ì‚° ì‹œì¥ì€ ë§¤ë¬¼ ì¦ê°€ë¡œ ê°€ê²© ìƒìŠ¹ì„¸ê°€ ë‹¤ì†Œ ì§„ì •ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "í•´ì™¸ëŠ” ì§€ì •í•™ì  ë³€ìˆ˜ì™€ ë¯¸êµ­ ì •ì¹˜ ì´ìŠˆê°€ í˜¼ì¬í–ˆìŠµë‹ˆë‹¤.",
            ]

        target_lines = [intro]
        target_lines.extend(core_lines)
        target_lines.append(outro)

        # pad or trim to 55 lines
        while len(target_lines) < 55:
            target_lines.insert(-1, "ì£¼ìš” ì´ìŠˆë¥¼ ì°¨ë¶„íˆ í™•ì¸í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.")
        if len(target_lines) > 75:
            target_lines = target_lines[:74] + [outro]

        tts_script["title"] = f"ì˜¤ëŠ˜ì˜ ëª¨ë‹ë‰´ìŠ¤ ({date_str_dash})"
        tts_script["lines"] = target_lines
        data["tts_script"] = tts_script
        data.setdefault("meta", {})
        data["meta"].update({
            "tts_fallback": True,
            "generated_at": datetime.now().isoformat(),
        })
        return data
    
    def _merge_holiday_cache(self, current_data):
        """
        íœ´ì¼ ë‹¤ìŒ ë‚ ì— ìºì‹œëœ ëª¨ë“  ë°ì´í„°ë¥¼ ë³‘í•©
        """
        now = datetime.now()
        merged_data = {
            **current_data,
            "hojae": [],
            "akjae": [],
            "holiday_merged_dates": []
        }
        
        # ìµœê·¼ 3ì¼ê°„ì˜ ìºì‹œ ë°ì´í„° ìˆ˜ì§‘
        for i in range(1, 4):  # 1ì¼ì „, 2ì¼ì „, 3ì¼ì „
            past_date = now - timedelta(days=i)
            date_str = past_date.strftime("%Y%m%d")
            cached_data = self.load_cached_data(date_str)
            
            if cached_data:
                merged_data["holiday_merged_dates"].append(date_str)
                
                # í˜¸ì¬ ë°ì´í„° ë³‘í•©
                for item in cached_data.get("hojae", []):
                    if item not in merged_data["hojae"]:
                        merged_data["hojae"].append(item)
                
                # ì•…ì¬ ë°ì´í„° ë³‘í•©
                for item in cached_data.get("akjae", []):
                    if item not in merged_data["akjae"]:
                        merged_data["akjae"].append(item)
        
        # í˜„ì¬ ë°ì´í„° ì¶”ê°€
        merged_data["hojae"].extend(current_data.get("hojae", []))
        merged_data["akjae"].extend(current_data.get("akjae", []))
        
        # ì¤‘ë³µ ì œê±°
        merged_data["hojae"] = list(set(merged_data["hojae"]))
        merged_data["akjae"] = list(set(merged_data["akjae"]))
        
        print(f"âœ… íœ´ì¼ ë°ì´í„° í†µí•© ì™„ë£Œ: {len(merged_data['holiday_merged_dates'])}ì¼ê°„ ë°ì´í„° ë³‘í•©")
        
        return merged_data
    
    def _clear_holiday_cache(self):
        """
        íœ´ì¼ ìºì‹œ ì •ë¦¬
        """
        now = datetime.now()
        
        # ìµœê·¼ 3ì¼ê°„ ìºì‹œ íŒŒì¼ ì •ë¦¬
        for i in range(1, 4):
            past_date = now - timedelta(days=i)
            date_str = past_date.strftime("%Y%m%d")
            cache_file = self.get_cache_filename(date_str)
            
            if os.path.exists(cache_file):
                try:
                    os.remove(cache_file)
                    print(f"ğŸ—‘ï¸ ìºì‹œ íŒŒì¼ ì •ë¦¬: {cache_file}")
                except Exception as e:
                    print(f"ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def generate_weekend_summary(self):
        """
        ì£¼ë§ì— ìŒ“ì¸ ë°ì´í„°ë¥¼ ì›”ìš”ì¼ ì•„ì¹¨ì— í•œ ë²ˆì— ì¶œë ¥
        Returns: ì£¼ë§ ì¢…í•© ìš”ì•½ ë°ì´í„°
        """
        print("ğŸ“… ì£¼ë§ ë°ì´í„° ì¢…í•© ë¶„ì„ ì‹œì‘...")
        
        # ì£¼ë§ ë‚ ì§œ ê³„ì‚° (ê¸ˆìš”ì¼ë¶€í„° ì¼ìš”ì¼ê¹Œì§€)
        now = datetime.now()
        
        # ì›”ìš”ì¼ì¸ì§€ í™•ì¸
        if now.weekday() != 0:  # 0ì€ ì›”ìš”ì¼
            print("âš ï¸ ì›”ìš”ì¼ì´ ì•„ë‹ˆì–´ì„œ ì£¼ë§ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì£¼ë§ ë‚ ì§œë“¤ ê³„ì‚°
        friday = now - timedelta(days=3)
        saturday = now - timedelta(days=2) 
        sunday = now - timedelta(days=1)
        
        weekend_dates = [
            friday.strftime("%Y%m%d"),
            saturday.strftime("%Y%m%d"),
            sunday.strftime("%Y%m%d")
        ]
        
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ì£¼ë§: {friday.strftime('%m/%d')}~{sunday.strftime('%m/%d')}")
        
        # ì£¼ë§ ë°ì´í„° ìˆ˜ì§‘
        weekend_data = []
        for date_str in weekend_dates:
            cached_data = self.load_cached_data(date_str)
            if cached_data:
                cached_data["date"] = date_str
                weekend_data.append(cached_data)
        
        if not weekend_data:
            print("ğŸ“­ ì£¼ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ì£¼ë§ ë°ì´í„° ì¢…í•© ë¶„ì„
        summary = self._analyze_weekend_data(weekend_data)
        
        return summary
    
    def _analyze_weekend_data(self, weekend_data):
        """
        ì£¼ë§ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìš”ì•½ ìƒì„±
        """
        print("ğŸ” ì£¼ë§ ë°ì´í„° ì¢…í•© ë¶„ì„ ì¤‘...")
        
        # ëª¨ë“  í˜¸ì¬/ì•…ì¬ ë°ì´í„° ìˆ˜ì§‘
        all_hojae = {}
        all_akjae = {}
        
        for day_data in weekend_data:
            date_str = day_data.get("date", "")
            
            # í˜¸ì¬ ë°ì´í„° ì²˜ë¦¬
            for item in day_data.get("hojae", []):
                if ":" in item:
                    company, reason = item.split(":", 1)
                    company = company.strip()
                    
                    if company not in all_hojae:
                        all_hojae[company] = {
                            "reasons": [],
                            "dates": [],
                            "count": 0
                        }
                    
                    all_hojae[company]["reasons"].append(reason.strip())
                    all_hojae[company]["dates"].append(date_str)
                    all_hojae[company]["count"] += 1
            
            # ì•…ì¬ ë°ì´í„° ì²˜ë¦¬
            for item in day_data.get("akjae", []):
                if ":" in item:
                    company, reason = item.split(":", 1)
                    company = company.strip()
                    
                    if company not in all_akjae:
                        all_akjae[company] = {
                            "reasons": [],
                            "dates": [],
                            "count": 0
                        }
                    
                    all_akjae[company]["reasons"].append(reason.strip())
                    all_akjae[company]["dates"].append(date_str)
                    all_akjae[company]["count"] += 1
        
        # ì£¼ë§ ìš”ì•½ ìƒì„±
        summary = {
            "weekend_dates": [data.get("date", "") for data in weekend_data],
            "hojae_summary": [],
            "akjae_summary": [],
            "top_hojae": [],
            "top_akjae": [],
            "total_hojae": len(all_hojae),
            "total_akjae": len(all_akjae)
        }
        
        # í˜¸ì¬ ìš”ì•½ (íšŒì‚¬ë³„ë¡œ ê·¸ë£¹í™”)
        for company, data in sorted(all_hojae.items(), key=lambda x: x[1]["count"], reverse=True):
            summary["hojae_summary"].append({
                "company": company,
                "count": data["count"],
                "reasons": data["reasons"],
                "dates": data["dates"]
            })
        
        # ì•…ì¬ ìš”ì•½
        for company, data in sorted(all_akjae.items(), key=lambda x: x[1]["count"], reverse=True):
            summary["akjae_summary"].append({
                "company": company,
                "count": data["count"],
                "reasons": data["reasons"],
                "dates": data["dates"]
            })
        
        # ìƒìœ„ í˜¸ì¬/ì•…ì¬ (3íšŒ ì´ìƒ ì–¸ê¸‰ëœ ê¸°ì—…)
        summary["top_hojae"] = [item for item in summary["hojae_summary"] if item["count"] >= 3]
        summary["top_akjae"] = [item for item in summary["akjae_summary"] if item["count"] >= 3]
        
        print(f"âœ… ì£¼ë§ ìš”ì•½ ì™„ë£Œ: í˜¸ì¬ {len(all_hojae)}ê°œ, ì•…ì¬ {len(all_akjae)}ê°œ")
        
        return summary
    
    def format_weekend_summary_message(self, weekend_summary):
        """
        ì£¼ë§ ìš”ì•½ì„ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ë¡œ í¬ë§·íŒ…
        """
        if not weekend_summary:
            return None
        
        dates = weekend_summary.get("weekend_dates", [])
        if dates:
            date_range = f"{dates[-1][-4:]}-{dates[-1][4:6]}-{dates[-1][6:8]} ~ {dates[0][-4:]}-{dates[0][4:6]}-{dates[0][6:8]}"
        else:
            date_range = "ì£¼ë§"
        
        lines = [
            f"ğŸ“Š ì£¼ë§ ì¢…í•© ë‰´ìŠ¤ ìš”ì•½ ({date_range})",
            "",
            f"ğŸ“ˆ í˜¸ì¬ ê¸°ì—…: {weekend_summary.get('total_hojae', 0)}ê°œ",
            f"ğŸ“‰ ì•…ì¬ ê¸°ì—…: {weekend_summary.get('total_akjae', 0)}ê°œ",
            ""
        ]
        
        # ìƒìœ„ í˜¸ì¬ ê¸°ì—…
        top_hojae = weekend_summary.get("top_hojae", [])
        if top_hojae:
            lines.append("ğŸ”¥ ì£¼ìš” í˜¸ì¬ ê¸°ì—…:")
            for item in top_hojae[:5]:  # ìƒìœ„ 5ê°œ
                lines.append(f"  â€¢ {item['company']}: {item['count']}íšŒ")
            lines.append("")
        
        # ìƒìœ„ ì•…ì¬ ê¸°ì—…
        top_akjae = weekend_summary.get("top_akjae", [])
        if top_akjae:
            lines.append("âš ï¸ ì£¼ìš” ì•…ì¬ ê¸°ì—…:")
            for item in top_akjae[:5]:  # ìƒìœ„ 5ê°œ
                lines.append(f"  â€¢ {item['company']}: {item['count']}íšŒ")
            lines.append("")
        
        lines.append("ğŸ“± ìì„¸í•œ ë‚´ìš©ì€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í™•ì¸í•˜ì„¸ìš”!")
        
        return "\n".join(lines)

    # NOTE: ê³¼ê±°ì— analyze_sentiment()ê°€ íŒŒì¼ ë‚´ì— 2ë²ˆ ì •ì˜ë˜ì–´ ì•„ë˜ìª½ì´ ìœ„ ë¡œì§ì„ ë®ì–´ì“°ë˜ ë¬¸ì œê°€ ìˆì—ˆìŒ.
    # í˜„ì¬ëŠ” ìœ„ì˜ analyze_sentiment() í•˜ë‚˜ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.

    def extract_hojae_list(self, briefing_data):
        """
        Extract hojae list from briefing data.
        Returns a list of hojae items.
        """
        if not briefing_data:
            return []
        
        return briefing_data.get("hojae", [])

    def extract_akjae_list(self, briefing_data):
        """
        Extract akjae list from briefing data.
        Returns a list of akjae items.
        """
        if not briefing_data:
            return []
        
        return briefing_data.get("akjae", [])

    def get_hojae_count(self, briefing_data):
        """
        Get the count of hojae items.
        """
        return len(self.extract_hojae_list(briefing_data))

    def get_akjae_count(self, briefing_data):
        """
        Get the count of akjae items.
        """
        return len(self.extract_akjae_list(briefing_data))

    def format_telegram_message(self, briefing_data, date_str=None, total_news_count=None):
        """
        Format telegram message for hojae notification.
        Returns a formatted message string.
        """
        hojae_list = self.extract_hojae_list(briefing_data)
        hojae_count = len(hojae_list)
        
        if not hojae_list:
            return None
        
        # Compose summary header (date, total news, hojae count)
        headline = "ğŸ“° ëª¨ë‹ë‰´ìŠ¤ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤"
        if date_str:
            headline += f" ({date_str})"

        summary_parts = []
        if total_news_count is not None:
            summary_parts.append(f"ì´ {total_news_count}ê±´ì˜ ë‰´ìŠ¤")
        summary_parts.append(f"í˜¸ì¬ ê¸°ì—…: {hojae_count}ê³³")
        summary_line = " / ".join(summary_parts)

        list_title = f"ğŸ“ˆ í˜¸ì¬ ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ({date_str})" if date_str else "ğŸ“ˆ í˜¸ì¬ ê¸°ì—… ë¦¬ìŠ¤íŠ¸"

        lines = [headline, summary_line, "", list_title]
        for item in hojae_list:
            lines.append(f"- {item}")
        message = "\n".join(lines)
        
        return message

    def get_section_summaries(self, briefing_data):
        """
        Extract section summaries from briefing data.
        """
        if not briefing_data:
            return {}
        
        return briefing_data.get("section_summaries", {})

    def has_sentiment_data(self, briefing_data):
        """
        Check if briefing data contains sentiment analysis (hojae/akjae).
        """
        if not briefing_data:
            return False
        
        return bool(briefing_data.get("hojae") or briefing_data.get("akjae"))

if __name__ == "__main__":
    # Test the SentimentAnalyzer
    analyzer = SentimentAnalyzer()
    
    print("ğŸ§ª SentimentAnalyzer í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ 1: ë‚ ì§œ íŒë‹¨ ë¡œì§
    print(f"ğŸ“… í˜„ì¬ ëª¨ë“œ: {analyzer.is_trading_day()}")
    
    # í…ŒìŠ¤íŠ¸ 2: ìºì‹œ ê¸°ëŠ¥
    test_data = {
        "hojae": ["ì‚¼ì„±ì „ì: ë°˜ë„ì²´ í˜¸ì¬", "LGí™”í•™: ë°°í„°ë¦¬ í˜¸ì¬"],
        "akjae": ["í˜„ëŒ€ì°¨: ë¦¬ì½œ ì•…ì¬"],
        "section_summaries": {"ì •ì¹˜": "í…ŒìŠ¤íŠ¸ ìš”ì•½"}
    }
    
    analyzer.save_cached_data(test_data, "20260117")
    loaded_data = analyzer.load_cached_data("20260117")
    print(f"âœ… ìºì‹œ í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if loaded_data else 'ì‹¤íŒ¨'}")
    
    # í…ŒìŠ¤íŠ¸ 3: ë°ì´í„° ë³‘í•©
    current_data = {
        "hojae": ["ì‚¼ì„±ì „ì: ì‹ ì œí’ˆ ì¶œì‹œ", "SKí•˜ì´ë‹‰ìŠ¤: ìˆ˜ì£¼"],
        "akjae": ["í˜„ëŒ€ì°¨: ë¦¬ì½œ ì•…ì¬", "ê¸°ì•„: ë¶€í’ˆ ê²°í•¨"]
    }
    
    merged = analyzer.merge_sentiment_data(current_data, test_data)
    print(f"âœ… ë³‘í•© í…ŒìŠ¤íŠ¸: í˜¸ì¬ {len(merged['hojae'])}ê°œ, ì•…ì¬ {len(merged['akjae'])}ê°œ")
    
    # í…ŒìŠ¤íŠ¸ 4: ì£¼ë§ ìš”ì•½ (ì›”ìš”ì¼ì´ ì•„ë‹ˆë©´ None ë°˜í™˜)
    weekend_summary = analyzer.generate_weekend_summary()
    print(f"ğŸ“… ì£¼ë§ ìš”ì•½ í…ŒìŠ¤íŠ¸: {'ì„±ê³µ' if weekend_summary else 'ì›”ìš”ì¼ì´ ì•„ë‹˜'}")
    
    print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
