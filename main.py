import os
import sys
import datetime
from rss_manager import RSSManager
from ai_processor import AIProcessor
from html_generator import HTMLGenerator
from sentiment_analyzer import SentimentAnalyzer
from data_cache import DataCache

from weather_manager import WeatherManager
from notifier import send_notification, send_telegram_hojae

# ì½˜ì†”ê³¼ íŒŒì¼ì— ë™ì‹œ ì¶œë ¥
class DualLogger:
    def __init__(self, file_path, mode='a'):
        self.file = open(file_path, mode, encoding='utf-8', buffering=1)
        self.console = sys.stdout
        
    def write(self, msg):
        self.console.write(msg)
        self.file.write(msg)
        self.console.flush()
        self.file.flush()
        
    def flush(self):
        self.console.flush()
        self.file.flush()

sys.stdout = DualLogger('run_job.log', 'a')
sys.stderr = DualLogger('run_job.log', 'a')

def main(send_push=True, use_cache=True):
    print("=== Morning News Bot Started ===")

    # ì‹œê°„ëŒ€ í†µì¼: GitHub ActionsëŠ” ê¸°ë³¸ UTCë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ, ëª¨ë“  íŒë‹¨/ìºì‹œ í‚¤ëŠ” KST ê¸°ì¤€ìœ¼ë¡œ ë§ì¶˜ë‹¤.
    kst = datetime.timezone(datetime.timedelta(hours=9))
    now_kst = datetime.datetime.now(kst)

    # í˜„ì¬ ì‹œê°„ í™•ì¸ (KST)
    current_hour = now_kst.hour
    is_morning_window = 8 <= current_hour < 9  # ì˜¤ì „ 8ì‹œ~9ì‹œ (KST)
    
    # ìºì‹œ ì‚¬ìš© ë¡œì§: ì˜¤ì „ 8-9ì‹œëŠ” ìƒˆë¡œ ìƒì„±, ê·¸ ì™¸ ì‹œê°„ì€ ìºì‹œ ì¬ì‚¬ìš©
    if is_morning_window:
        print(f"ğŸŒ… ì˜¤ì „ {current_hour}ì‹œ: ìƒˆë¡œìš´ ë‰´ìŠ¤ ìƒì„± ë° ìºì‹œ ì €ì¥")
        use_cache_for_loading = False  # ìƒˆë¡œ ìƒì„±
    else:
        print(f"ğŸ• {current_hour}ì‹œ: ìºì‹œëœ ë‰´ìŠ¤ ì¬ì‚¬ìš©")
        use_cache_for_loading = True   # ìºì‹œ ì¬ì‚¬ìš©
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
    is_test_mode = not send_push
    if is_test_mode and not is_morning_window:
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë°ì´í„° ìºì‹œ ì¬ì‚¬ìš© í™œì„±í™”")
    
    # Initialize cache system
    cache = DataCache()
    today_str = now_kst.strftime("%Y%m%d")
    
    # ìºì‹œ ìƒíƒœ í™•ì¸
    cache_status = cache.get_cache_status(today_str)
    print(f"ğŸ“Š ì˜¤ëŠ˜ì˜ ìºì‹œ ìƒíƒœ: RSS={cache_status['rss']}, AIë¶„ì„={cache_status['ai_analysis']}, ì¸ë¬¼={cache_status['key_persons']}")
    
    # 1. Setup (KST ê¸°ì¤€)
    date_str_dot = now_kst.strftime("%Y.%m.%d")
    date_str_file = now_kst.strftime("%Y%m%d")
    
    # Output Directory
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    main_filename = f"morning_news_{date_str_file}.html"
    main_file_path = os.path.join(output_dir, main_filename)
    
    # Initialize managers
    rss = RSSManager()
    ai = AIProcessor()
    sentiment = SentimentAnalyzer()
    html_gen = HTMLGenerator()
    wm = WeatherManager()
    
    # 2. Fetch Feeds & Weather (ì‹œê°„ ê¸°ë°˜ ìºì‹œ ë¡œì§)
    print("\n[Phase 1] Fetching RSS Feeds & Weather...")
    
    if use_cache and cache_status["rss"] and use_cache_for_loading:
        print("ğŸ”„ ìºì‹œëœ RSS ë°ì´í„° ë¡œë“œ ì¤‘...")
        all_news = cache.load_rss_data(today_str)
        if all_news:
            print(f"  - ìºì‹œëœ RSS ë¡œë“œ: {len(all_news)}ê±´")
        else:
            print("  - ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìˆ˜ì§‘...")
            all_news = rss.fetch_feeds()
            if use_cache:
                cache.save_rss_data(all_news, today_str)
    else:
        all_news = rss.fetch_feeds()
        if use_cache:
            cache.save_rss_data(all_news, today_str)
    
    weather_data = wm.get_weather()
    print(f"  - Total feeds fetched: {len(all_news)}")
    
    domestic_raw_all = [n for n in all_news if n['category'] == 'domestic']
    print(f"  - Total domestic articles: {len(domestic_raw_all)}")
    
    # Separate Science Times
    science_raw = [n for n in domestic_raw_all if "ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ" in n['source']]
    domestic_raw = [n for n in domestic_raw_all if "ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ" not in n['source']]
    print(f"  - Science articles: {len(science_raw)}")
    print(f"  - Non-science domestic articles: {len(domestic_raw)}")
    
    # Sort and limit Science Times to latest 10
    science_raw.sort(key=lambda x: x['published_dt'], reverse=True)
    science_raw = science_raw[:10]
    
    # Sort general news by date to ensure recent ones are prioritized across all sources
    domestic_raw.sort(key=lambda x: x['published_dt'], reverse=True)
    
    # ë°°ì¹˜ ì²˜ë¦¬: 200ê°œì”© ë‚˜ëˆ ì„œ AI ë¶„ë¥˜ í›„ ë³‘í•©
    batch_size = 200
    domestic_categorized_raw = {}
    
    # 3. AI Processing (ì‹œê°„ ê¸°ë°˜ ìºì‹œ ë¡œì§)
    print("\n[Phase 2] AI Processing...")
    
    if use_cache and cache_status["ai_analysis"] and use_cache_for_loading:
        print("ğŸ”„ ìºì‹œëœ AI ë¶„ì„ ë°ì´í„° ë¡œë“œ ì¤‘...")
        domestic_categorized_raw = cache.load_ai_analysis(today_str)
        if domestic_categorized_raw:
            print(f"  - ìºì‹œëœ AI ë¶„ì„ ë¡œë“œ: {sum(len(v) for v in domestic_categorized_raw.values())}ê±´")
        else:
            print("  - ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ë¶„ì„...")
            for batch_start in range(0, len(domestic_raw), batch_size):
                batch_end = min(batch_start + batch_size, len(domestic_raw))
                batch = domestic_raw[batch_start:batch_end]
                print(f"  - Processing batch: articles {batch_start+1}~{batch_end} ({len(batch)} articles)")
                
                batch_result = ai.process_domestic_news(batch)
                
                # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ë³‘í•©
                for category, items in batch_result.items():
                    if category not in domestic_categorized_raw:
                        domestic_categorized_raw[category] = []
                    domestic_categorized_raw[category].extend(items)
            if use_cache:
                cache.save_ai_analysis(domestic_categorized_raw, today_str)
    else:
        for batch_start in range(0, len(domestic_raw), batch_size):
            batch_end = min(batch_start + batch_size, len(domestic_raw))
            batch = domestic_raw[batch_start:batch_end]
            print(f"  - Processing batch: articles {batch_start+1}~{batch_end} ({len(batch)} articles)")
            
            batch_result = ai.process_domestic_news(batch)
            
            # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ë³‘í•©
            for category, items in batch_result.items():
                if category not in domestic_categorized_raw:
                    domestic_categorized_raw[category] = []
                domestic_categorized_raw[category].extend(items)
        if use_cache:
            cache.save_ai_analysis(domestic_categorized_raw, today_str)
    
    total_returned = sum(len(v) for v in domestic_categorized_raw.values())
    print(f"  - AI returned {total_returned} articles across {len(domestic_categorized_raw)} categories.")
    
    # Apply "At least 20" logic per category
    domestic_categorized = {}
    from config import config
    start_time = config.filter_start_time
    
    for category, items in domestic_categorized_raw.items():
        # Separate recent and older
        recent_items = [it for it in items if it['published_dt'] >= start_time]
        older_items = [it for it in items if it['published_dt'] < start_time]
        
        # Keep all items (remove minimum threshold)
        # If there are at least some recent items, prefer them over older ones
        if recent_items:
            domestic_categorized[category] = recent_items
        else:
            # Use older items only if no recent items exist
            domestic_categorized[category] = older_items
    
    # Count valid domestic items
    domestic_count = sum(len(items) for items in domestic_categorized.values())
    print(f"  - Classified {domestic_count} domestic articles (with fallback).")
    print(f"  - Domestic Categories: {list(domestic_categorized.keys())}")
 
    # 3.5. Extract Key Persons (ì‹œê°„ ê¸°ë°˜ ìºì‹œ ë¡œì§)
    print("\n[Phase 2.5] Extracting Key Persons...")
    
    if use_cache and cache_status["key_persons"] and use_cache_for_loading:
        print("ğŸ”„ ìºì‹œëœ ì£¼ìš” ì¸ë¬¼ ë°ì´í„° ë¡œë“œ ì¤‘...")
        key_persons = cache.load_key_persons(today_str)
        if key_persons:
            print(f"  - ìºì‹œëœ ì£¼ìš” ì¸ë¬¼ ë¡œë“œ: {len(key_persons)}ëª…")
        else:
            print("  - ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ì¶”ì¶œ...")
            key_persons = ai.extract_key_persons(domestic_categorized)
            if use_cache:
                cache.save_key_persons(key_persons, today_str)
    else:
        key_persons = ai.extract_key_persons(domestic_categorized)
        if use_cache:
            cache.save_key_persons(key_persons, today_str)
    
    if key_persons:
        print(f"  - Found {len(key_persons)} key persons:")
        for person_name, person_data in key_persons.items():
            print(f"    Â· {person_name} ({person_data['role']}): {person_data['count']}ê±´")
    else:
        print("  - No key persons found with 3+ articles")
 
    # 4. Generate Briefing (SentimentAnalyzerëŠ” í•­ìƒ ì‹¤í–‰)
    print("\n[Phase 3] Generating Morning Briefing...")
    briefing_data = sentiment.analyze_sentiment(domestic_categorized)
 
    # 5. Generate Main HTML
    print("\n[Phase 4] Generating Main HTML...")
    # Generate date-specific file
    html_gen.generate_main_page(
        domestic_categorized, 
        science_raw, 
        briefing_data,
        weather_data, 
        main_file_path, 
        date_str_dot,
        key_persons
    )
    
    # Also generate index.html in root folder (as a copy of the latest report)
    index_file_path = "index.html"
    html_gen.generate_main_page(
        domestic_categorized, 
        science_raw, 
        briefing_data,
        weather_data, 
        index_file_path, 
        date_str_dot,
        key_persons
    )

    # 5.5 í…”ë ˆê·¸ë¨ í˜¸ì¬ ê¸°ì—… ì•Œë¦¼ (ì„ íƒì )
    try:
        total_articles = domestic_count + len(science_raw)
        send_telegram_hojae(briefing_data, date_str_dot, total_articles)
    except Exception as e:
        print(f"âš ï¸ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹¤íŒ¨: {e}")
    
    # 6. Send Push Notification
    if send_push:
        print("\n[Phase 5] Sending Push Notification...")
        total_articles = domestic_count + len(science_raw)
        send_notification(date_str_dot, total_articles, main_filename)
    else:
        print("\n[Phase 5] (í…ŒìŠ¤íŠ¸ ëª¨ë“œ) ì•Œë¦¼ì€ ë°œì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("\n=== Finished Successfully ===")

if __name__ == "__main__":
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì: --no-push, --no-cache
    send_push = True
    use_cache = True
    
    for arg in sys.argv[1:]:
        if arg == "--no-push":
            send_push = False
        elif arg == "--no-cache":
            use_cache = False
    
    main(send_push, use_cache)
