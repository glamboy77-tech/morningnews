import os
import sys
import datetime
from rss_manager import RSSManager
from ai_processor import AIProcessor
from html_generator import HTMLGenerator
from sentiment_analyzer import SentimentAnalyzer
from data_cache import DataCache

from weather_manager import WeatherManager
from notifier import send_notification, send_telegram_hojae
from archive_generator import generate_archive
from retrofit_output_pages import retrofit_output_pages

# ì½˜ì†”ê³¼ íŒŒì¼ì— ë™ì‹œ ì¶œë ¥
class DualLogger:
    def __init__(self, file_path, mode='a'):
        self.file = open(file_path, mode, encoding='utf-8', buffering=1)
        self.console = sys.stdout
        
    def write(self, msg):
        self.console.write(msg)
        self.file.write(msg)
        self.console.flush()
        self.file.flush()
        
    def flush(self):
        self.console.flush()
        self.file.flush()

sys.stdout = DualLogger('run_job.log', 'a')
sys.stderr = DualLogger('run_job.log', 'a')

RUN_STATE_DIR = ".run_state"

def _ensure_run_state_dir():
    os.makedirs(RUN_STATE_DIR, exist_ok=True)

def get_done_marker_path(date_str: str) -> str:
    return os.path.join(RUN_STATE_DIR, f"done_{date_str}.json")

def has_done_marker(date_str: str) -> bool:
    return os.path.exists(get_done_marker_path(date_str))

def write_done_marker(date_str: str, payload: dict):
    _ensure_run_state_dir()
    marker_path = get_done_marker_path(date_str)
    tmp_path = f"{marker_path}.tmp"
    try:
        with open(tmp_path, "w", encoding="utf-8") as f:
            import json
            json.dump(payload, f, ensure_ascii=False, indent=2)
        os.replace(tmp_path, marker_path)
        print(f"âœ… done marker ìƒì„±: {marker_path}")
    except Exception as e:
        print(f"âš ï¸ done marker ìƒì„± ì‹¤íŒ¨: {e}")

def get_missing_required_outputs(date_str: str, output_path: str, cache: DataCache, sentiment: SentimentAnalyzer):
    missing = []
    if not cache.has_cache("rss", date_str):
        missing.append("data_cache/rss")
    if not cache.has_cache("ai_analysis", date_str):
        missing.append("data_cache/ai_analysis")
    if not cache.has_cache("key_persons", date_str):
        missing.append("data_cache/key_persons")
    if not os.path.exists(sentiment.get_cache_filename(date_str)):
        missing.append("sentiment_cache/sentiment")
    if not os.path.exists(output_path):
        missing.append("output/morning_news")
    return missing

def main(send_push=True, use_cache=True, *, ignore_done_marker: bool = False, tts_only: bool = False, scripts_only: bool = False):
    print("=== Morning News Bot Started ===")

    # ì‹œê°„ëŒ€ í†µì¼: GitHub ActionsëŠ” ê¸°ë³¸ UTCë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ, ëª¨ë“  íŒë‹¨/ìºì‹œ í‚¤ëŠ” KST ê¸°ì¤€ìœ¼ë¡œ ë§ì¶˜ë‹¤.
    kst = datetime.timezone(datetime.timedelta(hours=9))
    now_kst = datetime.datetime.now(kst)

    # Initialize cache system
    cache = DataCache()
    today_str = now_kst.strftime("%Y%m%d")

    # done marker ê¸°ë°˜ LLM ìŠ¤í‚µ (ìš´ì˜ ëª¨ë“œì—ì„œë§Œ)
    if use_cache and not ignore_done_marker and has_done_marker(today_str) and not scripts_only:
        required_missing = get_missing_required_outputs(
            today_str,
            os.path.join("output", f"morning_news_{today_str}.html"),
            cache,
            SentimentAnalyzer(),
        )
        if not required_missing:
            print(f"â­ï¸ done marker ë°œê²¬: {get_done_marker_path(today_str)}")
            print("ğŸš« ì˜¤ëŠ˜ì€ ì´ë¯¸ ì™„ë£Œë˜ì–´ LLM í˜¸ì¶œì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        print(f"âš ï¸ done markerëŠ” ìˆìœ¼ë‚˜ ëˆ„ë½ ì‚°ì¶œë¬¼ ì¡´ì¬: {', '.join(required_missing)}")
        print("â¡ï¸ ëˆ„ë½ ì‚°ì¶œë¬¼ ì¬ìƒì„±ì„ ìœ„í•´ ì‘ì—…ì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ìºì‹œ ìƒíƒœ í™•ì¸
    cache_status = cache.get_cache_status(today_str)
    print(f"ğŸ“Š ì˜¤ëŠ˜ì˜ ìºì‹œ ìƒíƒœ: RSS={cache_status['rss']}, AIë¶„ì„={cache_status['ai_analysis']}, ì¸ë¬¼={cache_status['key_persons']}")
    
    # 1. Setup (KST ê¸°ì¤€)
    date_str_dot = now_kst.strftime("%Y.%m.%d")
    date_str_file = now_kst.strftime("%Y%m%d")
    
    # Output Directory
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    
    main_filename = f"morning_news_{date_str_file}.html"
    main_file_path = os.path.join(output_dir, main_filename)
    
    # Initialize managers
    rss = RSSManager()
    ai = AIProcessor()
    sentiment = SentimentAnalyzer()
    html_gen = HTMLGenerator()
    wm = WeatherManager()
    
    if tts_only or scripts_only:
        print("\n[Phase 1~2.5] TTS-only ëª¨ë“œ: ìºì‹œëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        if not (cache_status["rss"] and cache_status["ai_analysis"] and cache_status["key_persons"]):
            print("âš ï¸ TTS-only ëª¨ë“œì— í•„ìš”í•œ ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì „ì²´ ì‹¤í–‰ìœ¼ë¡œ ìºì‹œë¥¼ ìƒì„±í•˜ì„¸ìš”.")
            print("   ì˜ˆ: python main.py --no-push")
            return
        print("ğŸ”„ ìºì‹œëœ RSS ë°ì´í„° ë¡œë“œ ì¤‘...")
        all_news = cache.load_rss_data(today_str) or []
        print("ğŸ”„ ìºì‹œëœ AI ë¶„ì„ ë°ì´í„° ë¡œë“œ ì¤‘...")
        domestic_categorized_raw = cache.load_ai_analysis(today_str) or {}
        print("ğŸ”„ ìºì‹œëœ ì£¼ìš” ì¸ë¬¼ ë°ì´í„° ë¡œë“œ ì¤‘...")
        key_persons = cache.load_key_persons(today_str) or {}
        weather_data = wm.get_weather()
    else:
        # 2. Fetch Feeds & Weather (ì‹œê°„ ê¸°ë°˜ ìºì‹œ ë¡œì§)
        print("\n[Phase 1] Fetching RSS Feeds & Weather...")
    
        if use_cache and cache_status["rss"]:
            print("ğŸ”„ ìºì‹œëœ RSS ë°ì´í„° ë¡œë“œ ì¤‘...")
            all_news = cache.load_rss_data(today_str)
            if all_news:
                print(f"  - ìºì‹œëœ RSS ë¡œë“œ: {len(all_news)}ê±´")
            else:
                print("  - ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìˆ˜ì§‘...")
                all_news = rss.fetch_feeds()
                if use_cache:
                    cache.save_rss_data(all_news, today_str)
        else:
            all_news = rss.fetch_feeds()
            if use_cache:
                cache.save_rss_data(all_news, today_str)
        
        weather_data = wm.get_weather()
        print(f"  - Total feeds fetched: {len(all_news)}")
        
        domestic_raw_all = [n for n in all_news if n['category'] == 'domestic']
        print(f"  - Total domestic articles: {len(domestic_raw_all)}")
        
        # Separate Science Times
        science_raw = [n for n in domestic_raw_all if "ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ" in n['source']]
        domestic_raw = [n for n in domestic_raw_all if "ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ" not in n['source']]
        print(f"  - Science articles: {len(science_raw)}")
        print(f"  - Non-science domestic articles: {len(domestic_raw)}")
        
        # Sort and limit Science Times to latest 10
        science_raw.sort(key=lambda x: x['published_dt'], reverse=True)
        science_raw = science_raw[:10]
        
        # Sort general news by date to ensure recent ones are prioritized across all sources
        domestic_raw.sort(key=lambda x: x['published_dt'], reverse=True)
        
        # ë°°ì¹˜ ì²˜ë¦¬: 200ê°œì”© ë‚˜ëˆ ì„œ AI ë¶„ë¥˜ í›„ ë³‘í•©
        batch_size = 200
        domestic_categorized_raw = {}
        
        # 3. AI Processing (ì‹œê°„ ê¸°ë°˜ ìºì‹œ ë¡œì§)
        print("\n[Phase 2] AI Processing...")
        
        if use_cache and cache_status["ai_analysis"]:
            print("ğŸ”„ ìºì‹œëœ AI ë¶„ì„ ë°ì´í„° ë¡œë“œ ì¤‘...")
            domestic_categorized_raw = cache.load_ai_analysis(today_str)
            if domestic_categorized_raw:
                print(f"  - ìºì‹œëœ AI ë¶„ì„ ë¡œë“œ: {sum(len(v) for v in domestic_categorized_raw.values())}ê±´")
            else:
                print("  - ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ë¶„ì„...")
                for batch_start in range(0, len(domestic_raw), batch_size):
                    batch_end = min(batch_start + batch_size, len(domestic_raw))
                    batch = domestic_raw[batch_start:batch_end]
                    print(f"  - Processing batch: articles {batch_start+1}~{batch_end} ({len(batch)} articles)")
                    
                    batch_result = ai.process_domestic_news(batch)
                    
                    # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ë³‘í•©
                    for category, items in batch_result.items():
                        if category not in domestic_categorized_raw:
                            domestic_categorized_raw[category] = []
                        domestic_categorized_raw[category].extend(items)
                if use_cache:
                    cache.save_ai_analysis(domestic_categorized_raw, today_str)
        else:
            for batch_start in range(0, len(domestic_raw), batch_size):
                batch_end = min(batch_start + batch_size, len(domestic_raw))
                batch = domestic_raw[batch_start:batch_end]
                print(f"  - Processing batch: articles {batch_start+1}~{batch_end} ({len(batch)} articles)")
                
                batch_result = ai.process_domestic_news(batch)
                
                # ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ë³‘í•©
                for category, items in batch_result.items():
                    if category not in domestic_categorized_raw:
                        domestic_categorized_raw[category] = []
                    domestic_categorized_raw[category].extend(items)
            if use_cache:
                cache.save_ai_analysis(domestic_categorized_raw, today_str)
        
        total_returned = sum(len(v) for v in domestic_categorized_raw.values())
        print(f"  - AI returned {total_returned} articles across {len(domestic_categorized_raw)} categories.")
        
        # Apply "At least 20" logic per category
        domestic_categorized = {}
        from config import config
        start_time = config.filter_start_time
        
        for category, items in domestic_categorized_raw.items():
            # Separate recent and older
            recent_items = [it for it in items if it['published_dt'] >= start_time]
            older_items = [it for it in items if it['published_dt'] < start_time]
            
            # Keep all items (remove minimum threshold)
            # If there are at least some recent items, prefer them over older ones
            if recent_items:
                domestic_categorized[category] = recent_items
            else:
                # Use older items only if no recent items exist
                domestic_categorized[category] = older_items
        
        # Count valid domestic items
        domestic_count = sum(len(items) for items in domestic_categorized.values())
        print(f"  - Classified {domestic_count} domestic articles (with fallback).")
        print(f"  - Domestic Categories: {list(domestic_categorized.keys())}")
    
        # 3.5. Extract Key Persons (ì‹œê°„ ê¸°ë°˜ ìºì‹œ ë¡œì§)
        print("\n[Phase 2.5] Extracting Key Persons...")
        
        if use_cache and cache_status["key_persons"]:
            print("ğŸ”„ ìºì‹œëœ ì£¼ìš” ì¸ë¬¼ ë°ì´í„° ë¡œë“œ ì¤‘...")
            key_persons = cache.load_key_persons(today_str)
            # NOTE: ë¹ˆ dict({})ë„ 'ì •ìƒ ë¡œë“œ(ì¸ë¬¼ ì—†ìŒ)'ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ None ì—¬ë¶€ë¡œ íŒë‹¨
            if key_persons is not None:
                print(f"  - ìºì‹œëœ ì£¼ìš” ì¸ë¬¼ ë¡œë“œ: {len(key_persons)}ëª…")
            else:
                print("  - ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ì¶”ì¶œ...")
                key_persons = ai.extract_key_persons(domestic_categorized)
                if key_persons is not None and use_cache:
                    cache.save_key_persons(key_persons, today_str)
        else:
            key_persons = ai.extract_key_persons(domestic_categorized)
            if key_persons is not None and use_cache:
                cache.save_key_persons(key_persons, today_str)
        
        if key_persons:
            print(f"  - Found {len(key_persons)} key persons:")
            for person_name, person_data in key_persons.items():
                print(f"    Â· {person_name} ({person_data['role']}): {person_data['count']}ê±´")
        else:
            print("  - No key persons found with 3+ articles")

    if tts_only or scripts_only:
        domestic_raw_all = [n for n in all_news if n.get('category') == 'domestic']
        science_raw = [n for n in domestic_raw_all if "ì‚¬ì´ì–¸ìŠ¤íƒ€ì„ì¦ˆ" in n.get('source', '')]
        science_raw.sort(key=lambda x: x['published_dt'], reverse=True)
        science_raw = science_raw[:10]

        domestic_categorized = {}
        from config import config
        start_time = config.filter_start_time
        for category, items in domestic_categorized_raw.items():
            recent_items = [it for it in items if it['published_dt'] >= start_time]
            older_items = [it for it in items if it['published_dt'] < start_time]
            domestic_categorized[category] = recent_items if recent_items else older_items
        domestic_count = sum(len(items) for items in domestic_categorized.values())
 
    # 4. Generate Briefing (SentimentAnalyzerëŠ” í•­ìƒ ì‹¤í–‰)
    print("\n[Phase 3] Generating Morning Briefing...")
    briefing_data = None
    if use_cache and os.path.exists(sentiment.get_cache_filename(today_str)):
        print("ğŸ”„ ìºì‹œëœ ê°ì„±/ë¸Œë¦¬í•‘ ë°ì´í„° ë¡œë“œ ì¤‘...")
        briefing_data = sentiment.load_cached_data(today_str)
    if tts_only or scripts_only:
        if briefing_data is None:
            print("âš ï¸ TTS-only ëª¨ë“œì¸ë° ì˜¤ëŠ˜ ê°ì„±/ë¸Œë¦¬í•‘ ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì „ì²´ ì‹¤í–‰ì„ ì§„í–‰í•˜ì„¸ìš”.")
            print("   ì˜ˆ: python main.py --no-push")
            return
        try:
            if scripts_only:
                briefing_data = sentiment.ensure_brief_scripts(
                    briefing_data,
                    today_str,
                    categorized_news=domestic_categorized,
                    max_retries=3,
                )
            else:
                briefing_data = sentiment.regenerate_tts_only(
                    briefing_data,
                    today_str,
                    categorized_news=domestic_categorized,
                    max_retries=3,
                )
            if use_cache:
                sentiment.save_cached_data(briefing_data, today_str)
        except Exception as e:
            print(f"âš ï¸ TTS-only ì¬ìƒì„± ì‹¤íŒ¨: {e}")
            return
    elif briefing_data is None:
        # ë‹¹ì¼ ë¸Œë¦¬í•‘/ìŠ¤í¬ë¦½íŠ¸ê°€ ë°˜ë“œì‹œ ìƒì„±ë˜ë„ë¡ stale ìºì‹œ ì‚¬ìš© ê¸ˆì§€
        briefing_data = sentiment.analyze_sentiment(
            domestic_categorized,
            today_str,
            use_cache=use_cache,
            allow_stale=False,
            max_retries=3,
        )

    # Save YouTube TTS script (separate text file) - always on
    tts_path = sentiment.save_tts_script_text(briefing_data, today_str)
    if tts_path:
        tts_lines = briefing_data.get("tts_script", {}).get("lines", []) if briefing_data else []
        print(f"âœ… TTS ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì™„ë£Œ: {tts_path} (lines={len(tts_lines)})")
    else:
        print("âš ï¸ TTS ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")

    # Save brief source_script JSON (single output)
    brief_scripts = briefing_data.get("brief_scripts") if isinstance(briefing_data, dict) else None
    if isinstance(brief_scripts, dict) and isinstance(brief_scripts.get("source_script"), str):
        brief_path = sentiment.save_brief_scripts_json(brief_scripts, today_str)
        if brief_path:
            print(f"âœ… ë¸Œë¦¬í”„ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì™„ë£Œ: {brief_path}")
        else:
            print("âš ï¸ ë¸Œë¦¬í”„ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ì‹¤íŒ¨")

        keyword_path = sentiment.save_keywords_text(brief_scripts.get("keywords"), today_str)
        if keyword_path:
            print(f"âœ… í‚¤ì›Œë“œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {keyword_path}")
        else:
            print("âš ï¸ í‚¤ì›Œë“œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ë˜ëŠ” í‚¤ì›Œë“œ ë°ì´í„° ì—†ìŒ")

    if scripts_only:
        print("\n=== Finished Scripts-Only Successfully ===")
        return
 
    # 5. Generate Main HTML
    print("\n[Phase 4] Generating Main HTML...")
    if not os.path.exists(main_file_path):
        # Generate date-specific file
        html_gen.generate_main_page(
            domestic_categorized, 
            science_raw, 
            briefing_data,
            weather_data, 
            main_file_path, 
            date_str_dot,
            key_persons
        )
        
        # Also generate index.html in root folder (as a copy of the latest report)
        index_file_path = "index.html"
        html_gen.generate_main_page(
            domestic_categorized, 
            science_raw, 
            briefing_data,
            weather_data, 
            index_file_path, 
            date_str_dot,
            key_persons
        )
    else:
        print(f"âœ… ì˜¤ëŠ˜ì HTML ì´ë¯¸ ì¡´ì¬: {main_file_path}")

    # 5.0 Retrofit old output pages for GitHub Pages subpath + Archive nav
    try:
        print("\n[Phase 4.0] Retrofitting old output pages...")
        changed = retrofit_output_pages(output_dir)
        print(f"  - Retrofitted files: {changed}")
    except Exception as e:
        print(f"âš ï¸ output í˜ì´ì§€ ë³´ì • ì‹¤íŒ¨: {e}")

    # 5.1 Generate archive page (list of previous daily HTML files)
    try:
        print("\n[Phase 4.1] Generating Archive Page...")
        generate_archive(output_dir=output_dir, archive_path="archive.html")
    except Exception as e:
        print(f"âš ï¸ archive.html ìƒì„± ì‹¤íŒ¨: {e}")

    # 5.5 í…”ë ˆê·¸ë¨ í˜¸ì¬ ê¸°ì—… ì•Œë¦¼ (ì„ íƒì )
    try:
        total_articles = domestic_count + len(science_raw)
        send_telegram_hojae(briefing_data, date_str_dot, total_articles)
    except Exception as e:
        print(f"âš ï¸ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹¤íŒ¨: {e}")
    
    # 6. Send Push Notification
    if send_push:
        print("\n[Phase 5] Sending Push Notification...")
        total_articles = domestic_count + len(science_raw)
        send_notification(date_str_dot, total_articles, main_filename)
    else:
        print("\n[Phase 5] (í…ŒìŠ¤íŠ¸ ëª¨ë“œ) ì•Œë¦¼ì€ ë°œì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("\n=== Finished Successfully ===")

    # done marker ìƒì„± (ëª¨ë“  í•„ìˆ˜ ì‚°ì¶œë¬¼ ì™„ë¹„ ì‹œì—ë§Œ)
    if use_cache:
        missing = get_missing_required_outputs(today_str, main_file_path, cache, sentiment)
        if not missing:
            write_done_marker(
                today_str,
                {
                    "date": today_str,
                    "created_at": now_kst.isoformat(),
                    "output": main_file_path,
                },
            )
        else:
            print(f"âš ï¸ done marker ë³´ë¥˜: ëˆ„ë½ëœ ì‚°ì¶œë¬¼ -> {', '.join(missing)}")

if __name__ == "__main__":
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì: --no-push, --no-cache, --ignore-done-marker, --tts-only, --scripts-only
    send_push = True
    use_cache = True
    ignore_done_marker = False
    tts_only = False
    scripts_only = False
    
    for arg in sys.argv[1:]:
        if arg == "--no-push":
            send_push = False
        elif arg == "--no-cache":
            use_cache = False
        elif arg == "--ignore-done-marker":
            ignore_done_marker = True
        elif arg == "--tts-only":
            tts_only = True
        elif arg == "--scripts-only":
            scripts_only = True
    
    main(
        send_push,
        use_cache,
        ignore_done_marker=ignore_done_marker,
        tts_only=tts_only,
        scripts_only=scripts_only,
    )
